<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="HTJxFQYrIX3p9c68oCi3cp22YWIGnLSAAD3wrcxR4zQ"> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Soft Actor-Critic | Wooyeol Lee</title> <meta name="author" content="Wooyeol Lee"> <meta name="description" content="Review on " soft actor-critic off-policy maximum entropy deep rl with a stochastic actor> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, RL, Reinforcement, Learning, AI, Robot, work-out, coffee, travel"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9C%88%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://thisiswooyeol.github.io/projects/SoftActorCritic/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Wooyeol Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Soft Actor-Critic</h1> <p class="post-description">Review on "Soft Actor-Critic; Off-Policy Maximum Entropy Deep RL with a Stochastic Actor"</p> </header> <article> <h1 id="tldr">TL;DR:</h1> <ul> <li>Soft Actor-Critic; <strong>an off-policy actor-critic deep RL algorithm</strong> based on the <strong>maximum entropy reinforcement learning framework</strong> </li> <li>SAC achieves SOTA performance on a range of continuous control benchmark tests, outperforming prior on-policy and off-policy method.</li> <li>SAC is very <strong>stable</strong> in contrast to other off-policy algorithms. <br> <br> <br> </li> </ul> <hr> <h1 id="introduction">Introduction</h1> <p><br></p> <h3 id="challenges-on-model-free-deep-reinforcement-learning-rl-algorithms">Challenges on Model-free deep reinforcement learning (RL) algorithms</h3> <ol> <li>Model-free deep RL methods are notoriously expensive in terms of their <strong>sample complexity</strong>.</li> <li>Model-free deep RL methods are often <strong>brittle with respect to their hyperparameters</strong>.</li> </ol> <p>One cause for the poor sample efficiency is <strong>on-policy learning</strong> such as TRPO, PPO or A3C. In contrast to on-policy learning, <strong>off-policy algorithms</strong> aim to reuse past experience. Unfortunately, the combination of <strong>off-policy learning</strong> and <strong>high dimensional, nonlinear function approximation with neural networks</strong> presents a major challenge for <strong>stability and convergence</strong>(so-called deadly triad). This challenge is further exacerbated in continuous state and action spaces, where DDPG is in such settings. DDPG provides for <strong>sample-efficient learning</strong> but is notoriously challenging to use due to its <strong>extreme brittleness and hyperparameter sensitivity.</strong></p> <p><br> <br></p> <h3 id="maximum-entropy-framework">Maximum Entropy Framework</h3> <p>To solve these problems, the maximum entropy framework is used in this paper. The maximum entropy formulation provides a <strong>substantial improvement in exploration and robustness</strong> with a <em>stochastic</em> actor. Prior work has proposed off-policy algorithms based on <a href="https://thisiswooyeol.github.io/projects/SoftQLearning/"><code class="language-plaintext highlighter-rouge">soft Q-learning</code></a> and its variants. However, the off-policy variants require <strong>complex approximate inference procedures in continuous action spaces</strong>(sampling network in Soft Q-Learning). In this paper, off-policy maximum entropy actor-critic algorithm (SAC) is devised. SAC avoids the complexity and potential instability associated with <strong>approximate inference in prior off-policy maximum entropy algorithms based on soft Q-learning.</strong></p> <p><br> <br></p> <h3 id="is-soft-q-learning-is-true-actor-critic-algorithm">Is Soft Q-learning is true actor-critic algorithm?</h3> <p>Although the soft Q-learning algorithm has a value function and actor network, <strong>it is not a true actor-critic algorithm</strong>: the Q-function is estimating the optimal Q-function, and <strong>the actor does not directly affect the Q-function except through the data distribution.</strong> Hence, soft Q-learning motivates the actor network as an <strong>approximate sampler</strong> rather than the actor in an actor-critic algorithm. While convergence of soft Q-learning depends on <strong>how well this sampler approximates the true posterior</strong>, SAC converges to the optimal policy <strong>from a given policy class, regardless of the policy parameterization.</strong></p> <p><br> <br> <br></p> <hr> <h1 id="preliminaries">Preliminaries</h1> <p><br></p> <h3 id="maximum-entropy-reinforcement-learning">Maximum Entropy Reinforcement Learning</h3> <p>Maximum entropy objective with the expected entropy of the policy over \(\rho_\pi(s_t)\) :</p> \[\begin{equation} J(\pi) =\sum_{t=0}^\infty \mathbb E_{(s_t,a_t) \sim \rho_\pi} \left[\sum_{l=t}^\infty \gamma^{l-t} \mathbb E_{s_l \sim p,a_l \sim \pi} \left[r(s_t,a_t)+\alpha \mathcal H(\pi(\cdot|s_t)) \right]\right]. \end{equation}\] <p>The temperature parameter \(\alpha\) determines the relative importance of the entropy term against the reward, and thus controls the stochasticity of the optimal policy. By adapting maximum entropy framework, the policy is <strong>incentivized to explore more widely, while giving up on clearly unpromising avenues</strong>. Also, the policy can <strong>capture multiple modes of near optimal behavior</strong> and <strong>improve learning speed over state-of-art methods.</strong></p> <p><br> <br> <br></p> <hr> <h1 id="from-soft-policy-iteration-to-soft-actor-critic">From Soft Policy Iteration to Soft Actor-Critic</h1> <p><br></p> <h3 id="derivation-of-soft-policy-iteration">Derivation of Soft Policy Iteration</h3> <p>Derivation of sof policy iteration is based on a <strong>tabular setting</strong>, to enable theoretical analysis and convergence guarantees, and this method will extends into the general continuous setting in the next section. This paper shows that soft policy iteration converges to the optimal policy <strong>within a set of policies</strong> which might correspond, for instance, to a set of parameterized densities.</p> <p>Letâ€™s begin with defining the <strong>soft Bellman backup operator</strong> \(\mathcal T^\pi\) as</p> \[\begin{equation}\label{eqn:soft-bell-op} \mathcal T^\pi Q(s_t,a_t) \triangleq r(s_t,a_t)+ \gamma \mathbb E_{s_{t+1} \sim p} \left[ V(s_{t+1})\right] , \end{equation}\] <p>where</p> \[\begin{equation} V(s_t)=\mathbb E_{a_t \sim \pi} \left[ Q(s_t,a_t)-\alpha \mathrm{log} \pi (a_t \mid s_t)\right] \end{equation}\] <p>is the soft state value function. Use this definition, policy evaluation step can be defined.</p> <blockquote> <p><strong>Lemma 1</strong> (Soft Policy Evaluation). <em>Consider the soft Bellman backup operator \(\mathcal T^\pi\) in Equation \ref{eqn:soft-bell-op} and a mapping \(Q^0: \mathcal{S \times A} \to \mathbb R\) with \(\left\vert \mathcal A \right\vert &lt; \infty\) , and define \(Q^{k+1}= \mathcal T^\pi Q^k\) . Then the sequence \(Q^k\) will converge to the soft Q-value of \(\pi\) as \(k \to \infty\) .</em></p> </blockquote> <p>How to prove Lemma 1:</p> <ul> <li>Check Appendix B.1</li> <li>The assumption \(\left\vert \mathcal A \right\vert &lt; \infty\) is required to guarantee the entropy augmented reward is bounded. (Actually Iâ€™m wondering why it is neededâ€¦)</li> </ul> <p><br> In the policy improvement step, SAC updates the policy towards the exponential of the new Q-function. In contrast to SQL, SAC <strong>restrict the policy to some set of parameterizable policies \(\Pi\) that is tractable</strong> such as a parameterized family of Gaussians. By using the information projection, policy is updated according to</p> \[\begin{equation}\label{eqn:policy-proj} \pi_\mathrm{new} = \underset{\pi ' \in \Pi}{\mathrm{argmin}} \mathrm{D_{KL}} \left( \pi' (\cdot|s_t) \parallel \frac{\mathrm{exp} (Q^{\pi_{\mathrm{old}}} (s_t, \cdot))}{Z^{\pi_{\mathrm{old}}} (s_t)} \right). \end{equation}\] <p>For this projection, we can show that the new, projected policy has a higher value than the old policy with respect to the objective (Lemma 2).</p> <blockquote> <p><strong>Lemma 2</strong> (Soft Policy Improvement). <em>Let \(\pi_\mathrm{old} \in \Pi\) and let \(\pi_\mathrm{new}\) be the optimizer of the minimization problem defined in Equation \ref{eqn:policy-proj}. Then \(Q^{\pi_\mathrm{new}}(s_t,a_t) \geq Q^{\pi_\mathrm{old}}(s_t,a_t)\) for all \((s_t,a_t) \in \mathcal{S \times A}\) with \(\left\vert \mathcal A \right\vert &lt; \infty\) .</em></p> </blockquote> <p>How to prove Lemma 2: Check Appendix B.2</p> <p><br> Alternating the soft policy evaluation and the soft policy improvement steps will converge to the optimal maximum entropy policy among the policies in \(\Pi\).</p> <blockquote> <p><strong>Theorem 1</strong> (Soft Policy Iteration). <em>Repeated application of soft policy evaluation and soft policy improvement from any \(\pi \in \Pi\) converges to a policy</em> \(\pi^*\) <em>such that</em> \(Q^{\pi^*}(s_t,a_t) \geq Q^\pi(s_t,a_t)\) <em>for all \(\pi \in \Pi\) and \((s_t,a_t) \in \mathcal{S \times A}\) , assuming \(\left\vert \mathcal A \right\vert &lt; \infty\) .</em></p> </blockquote> <p>How to prove Theorem 1: Check Appendix B.3</p> <p><br> <br></p> <h3 id="soft-actor-critic">Soft Actor-Critic</h3> <p>Soft Actor-Critic algorithm alternates between optimizing the Q-function and the policy networks with stochastic gradient descent instead of running evaluation and improvement to converge. Parameterized networks \(V_\psi (s_t), Q_\theta (s_t,a_t)\) , (<strong>a tractable policy</strong>) \(\pi_\phi (a_t \mid s_t)\) are used. For Q-function, SAC makes use of two Q-functions \(Q_{\theta_1} (s_t,a_t), Q_{\theta_2} (s_t,a_t)\) to mitigate positive bias, which is found to speed up training. In particular, minimum of the Q-functions is used for the value gradient and policy gradient.</p> <p>The soft value function which is included to stabilize training is trained to minimize the squared residual error</p> \[\begin{equation}\label{eqn:val-err} J_V(\psi) = \mathbb E_{s_t \sim \mathcal D} \left[ \frac{1}{2} \left( V_\psi (s_t) - \mathbb E_{a_t \sim \pi_\phi} \left[ \underset{ i \in \left\lbrace 1,2 \right\rbrace }{\mathrm{min}} \ Q_{\theta_i}(s_t,a_t) - \mathrm{log} \ \pi_\phi (a_t \mid s_t) \right] \right)^2 \right]. \end{equation}\] <p>The gradient of Equation \ref{eqn:val-err} can be estimated with an unbiased estimator</p> \[\begin{equation} \hat{\nabla}_ \psi J_V(\psi) = \nabla_\psi V_\psi (s_t) \left( V_\psi (s_t) - \underset{ i \in \left\lbrace 1,2 \right\rbrace }{\mathrm{min}} \ Q_{\theta_i}(s_t,a_t) + \mathrm{log} \ \pi_\phi (a_t \mid s_t) \right). \end{equation}\] <p>Note that <strong>the actions are sampled from the current policy.</strong> (To measure the entropy of the current policy.) The soft Q-function is trained to minimize the soft Bellman residual</p> \[\begin{equation} J_Q (\theta_i) = \mathbb E_{(s_t,a_t) \sim \mathcal D} \left[ \frac{1}{2} \left( Q_{\theta_i} (s_t,a_t) - \left(r(s_t,a_t) + \gamma \mathbb E_{s_{t+1} \sim p} \left[ V_\bar{\psi} (s_{t+1}) \right] \right) \right)^2 \right], \end{equation}\] <p>which again can be optimized with stochastic gradients</p> \[\begin{equation} \hat{\nabla}_ {\theta_i} J_Q(\theta_i) = \nabla_{\theta_i} Q_{\theta_i}(s_t,a_t) \left( Q_{\theta_i} (s_t,a_t) - r(s_t,a_t) - \gamma V_\bar{\psi} (s_{t+1}) \right). \end{equation}\] <p>\(V_\bar{\psi}\) is a target value network that \(\bar{\psi}\) can be an exponential moving average of \(\psi\) (in Algorithm 1) or can be a periodically updated parameters of \(\psi\) (as in Appendix E). Finally, the policy parameters can be learned by directly minimizing the expected KL-divergence in Equation \ref{eqn:policy-proj}:</p> \[\begin{equation}\label{eqn:policy-obj} J_\pi (\phi) = \mathbb E_{s_t \sim \mathcal D} \left[ \mathrm{D_{KL}} \left( \pi' (\cdot|s_t) \parallel \frac{\mathrm{exp} (Q_\theta (s_t, \cdot))}{Z_\theta (s_t)} \right) \right]. \end{equation}\] <p>To minimize \(J_\pi\) with gradient descent, SAC <strong>reparameterize the policy using a neural network transformation</strong></p> \[\begin{equation} a_t = f_\phi(\epsilon_t;s_t), \end{equation}\] <p>where \(\epsilon_t\) is an <strong>input noise vector</strong>, sampled from some fixed distribution, such as spherical Gaussian. How and why policy network is reparameterized can be described as below.</p> <div class="row justify-content-center"> <div class="col-10"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/SoftActorCritic/SAC-comp-graph.PNG-480.webp 480w, /assets/img/SoftActorCritic/SAC-comp-graph.PNG-800.webp 800w, /assets/img/SoftActorCritic/SAC-comp-graph.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/SoftActorCritic/SAC-comp-graph.PNG" class="img-fluid" width="100%" height="auto" title="SAC-comp-graph" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparison on computational graph of the policy network with and without reparameterization trick. Left graph is the policy network without reparameterization trick. It samples an action directly from the state conditioned distribution. Right graph is the policy network with reparameterization trick. It samples an action from standard normal distribution and rescales it to fit into the actual distribution. </div> <p>If we sample actions as the left graph does, the backpropagation of the objective will be stopped at the sampling node. Therefore, by separating the sampling node as the right graph does, we can use backpropagation to update the policy parameters \(\phi\). Now, we can rewrite the objective in Equation \ref{eqn:policy-obj} as</p> \[\begin{equation} J_\pi(\phi) = \mathbb E_{s_t \sim \mathcal D, \epsilon \sim \mathcal N} \left[ \mathrm{log} \ \pi_\phi(f_\phi(\epsilon_t;s_t) \mid s_t) - \underset{ i \in \left\lbrace 1,2 \right\rbrace }{\mathrm{min}} \ Q_{\theta_i}(s_t,f_\phi(\epsilon_t;s_t)) \right], \end{equation}\] <p>which can be optimized with unbiased gradient estimator</p> \[\begin{equation} \hat{\nabla}_ \phi J_\pi(\phi) = \nabla_ \phi \mathrm{log} \ \pi_\phi(a_t \mid s_t) + \left( \nabla_ {a_t} \mathrm{log} \ \pi_\phi (a_t \mid s_t) - \nabla_ {a_t} \underset{ i \in \left\lbrace 1,2 \right\rbrace }{\mathrm{min}} \ Q_{\theta_i}(s_t,a_t) \right) \nabla_ \phi f_\phi(\epsilon_t;s_t), \end{equation}\] <p>where \(a_t\) is evaluated at \(f_\phi(\epsilon_t;s_t)\).</p> <p>The complete algorithm is described in Algorithm 1.</p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/SoftActorCritic/SAC-algorithm.PNG-480.webp 480w, /assets/img/SoftActorCritic/SAC-algorithm.PNG-800.webp 800w, /assets/img/SoftActorCritic/SAC-algorithm.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/SoftActorCritic/SAC-algorithm.PNG" class="img-fluid" width="100%" height="auto" title="SAC-algorithm" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL with a Stochastic Actor </div> <p><br> <br> <br></p> <hr> <h1 id="experiments">Experiments</h1> <p>The goal of experimental evaluation is comparing the <strong>sample complexity</strong> and <strong>stability</strong> of SAC with prior on/off-policy deep RL algorithms on continuous control tasks. DDPG, PPO, SQL, and TD3 are used for comparison, where exploration noise of DDPG and PPO is turned off and only the mean action for SAC is used in evaluation rollouts. (remove any stochasticity and noise of algorithm in order for the algorithms can fully exploit their knowledge)</p> <p><br></p> <h3 id="comparative-evaluation">Comparative Evaluation</h3> <div class="row justify-content-xl-center"> <div class="col-10"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/SoftActorCritic/SAC-comparative-eval.PNG-480.webp 480w, /assets/img/SoftActorCritic/SAC-comparative-eval.PNG-800.webp 800w, /assets/img/SoftActorCritic/SAC-comparative-eval.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/SoftActorCritic/SAC-comparative-eval.PNG" class="img-fluid" width="100%" height="auto" title="SAC-Eval1" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL with a Stochastic Actor </div> <p>SAC performs comparably to the baseline methods on the easier tasks and outperforms them on the harder tasks with a large margin, both in terms of learning speed(vs. PPO, SQL especially) and the final performance(vs. DDPG, SQL especially).</p> <p><br> <br></p> <h3 id="ablation-study">Ablation study</h3> <p>To check which particular components of SAC are important for good performance and how sensitive SAC is to some of the most important hyperparameters. <br></p> <p><strong>1. Stochastic vs. deterministic policy</strong></p> <div class="row justify-content-xl-center"> <div class="col-xl-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/SoftActorCritic/SAC-deterministic-vs-stochastic.PNG-480.webp 480w, /assets/img/SoftActorCritic/SAC-deterministic-vs-stochastic.PNG-800.webp 800w, /assets/img/SoftActorCritic/SAC-deterministic-vs-stochastic.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/SoftActorCritic/SAC-deterministic-vs-stochastic.PNG" class="img-fluid" width="100%" height="auto" title="SAC-Eval2" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL with a Stochastic Actor </div> <p>Comparison of SAC and a deterministic variant of SAC (closely resembles DDPG, with the exception of 2 Q-func., hard target updates, no target actor, fixed exploration noise) shows that <strong>learning a stochastic policy with entropy maximization can drastically stabilize training</strong>. <br></p> <p><strong>2. Policy evaluation</strong></p> <p>Deterministic evaluation (choosing the mean of the policy distribution at evaluation rollouts) can yield better performance than stochastic evaluation. <br></p> <p><strong>3. Reward scale</strong></p> <p>Soft actor-critic is particularly <strong>sensitive to the scaling of the reward signal</strong>, because it serves the role of the temperature of the energy-based optimal policy (\(\propto 1/\alpha\)) and thus <strong>controls its stochasticity</strong>.</p> <table> <thead> <tr> <th>Reward magnitudes</th> <th>Policy</th> <th>Problem</th> </tr> </thead> <tbody> <tr> <td>Small</td> <td>nearly uniform</td> <td>fail to exploit the reward signal</td> </tr> <tr> <td>Large</td> <td>nearly deterministic</td> <td>lack of adequate exploration</td> </tr> </tbody> </table> <p><br></p> <p>They found that <strong>reward scale to be the only hyperparameter that requires tuning</strong>. The need of manual hyperparameter tuning leads to the follow up study of <a href="https://thisiswooyeol.github.io/projects/SoftActorCritic-autoHO/"><code class="language-plaintext highlighter-rouge">Automating Entropy Adjustment for Maximum Entropy RL</code></a>. <br></p> <p><strong>4. Target network update</strong></p> <p>Using a <strong>separate target value network</strong> that slowly tracks the actual value function <strong>improves stability</strong>. They found that the range of suitable values of \(\tau\) to be relatively wide.</p> <p><br> The results of experiment 2 - 4 are described below.</p> <div class="row justify-content-xl-center"> <div class="col-xl-10"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/SoftActorCritic/SAC-hyperparams-eval.PNG-480.webp 480w, /assets/img/SoftActorCritic/SAC-hyperparams-eval.PNG-800.webp 800w, /assets/img/SoftActorCritic/SAC-hyperparams-eval.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/SoftActorCritic/SAC-hyperparams-eval.PNG" class="img-fluid" width="100%" height="auto" title="SAC-Eval3" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL with a Stochastic Actor </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2024 Wooyeol Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-S9X10ME12Y"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-S9X10ME12Y");</script> <script async src="https://rum.cronitor.io/script.js"></script> <script>window.cronitor=window.cronitor||function(){(window.cronitor.q=window.cronitor.q||[]).push(arguments)},cronitor("config",{clientKey:""});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>