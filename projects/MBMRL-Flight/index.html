<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/><meta name="msvalidate.01" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>MBMRL for Flight | Wooyeol Lee</title> <meta name="author" content="Wooyeol Lee"/> <meta name="description" content="Review on "Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads""/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, RL, Reinforcement, Learning, AI, Robot, work-out, coffee, travel"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>✈️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://thisiswooyeol.github.io/projects/MBMRL-Flight/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Wooyeol Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">MBMRL for Flight</h1> <p class="post-description">Review on "Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads"</p> </header> <article> <p>TL;DR:</p> <ul> <li>Proposed a <strong>meta-learning approach</strong> that “learns how to learn” <strong>models of various payloads</strong> that a priori unknown physical properties vary dynamics.</li> <li>By <strong>augmenting the dynamic model with stochastic latent variables</strong>, the authors infused meta-learning approach into MBRL.</li> <li> <strong>Usage of unknown latent variables</strong> leads to outperforming results compared to pure model-based methods.</li> <li>By <strong>online adaptation mechanism</strong>, the dynamics variables converges and the tracking error also reduces.</li> <li>Proposed approach shows improved performance for the full end-to-end payload trasportation task as well as transporting a suspended payload towards a moving target, around an obstacle by following a predefined path, and along trajectories dictated using a “wand”-like interface.</li> </ul> <p><br> <br> <br></p> <hr> <h1 id="introduction">Introduction</h1> <p><br></p> <h3 id="task-definition">Task definition</h3> <p><br></p> <div class="row justify-content-center"> <div class="col-10"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MBMRL-Flight/MBMRL-Flight-transport-task.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MBMRL-Flight/MBMRL-Flight-transport-task.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MBMRL-Flight/MBMRL-Flight-transport-task.PNG-1400.webp"></source> <img src="/assets/img/MBMRL-Flight/MBMRL-Flight-transport-task.PNG" class="img-fluid" width="auto" height="auto" title="Quadcopter Payload Transport Task" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads" </div> <p><br></p> <h3 id="limits-of-hand-designed-model-on-the-flight-tasks">Limits of hand-designed model on the flight tasks</h3> <p><strong>The complex interaction between the magnetic gripper and the payload</strong> are unlikely to be represented accurately by hand-designed models. Even more unpredictable is <strong>the effect of the payload on the dynamics of the quadcopter</strong> when the payload is lifted off the ground.</p> <p><br></p> <h3 id="need-for-fast-adaptation">Need for fast adaptation</h3> <p>Conventional learning-based methods typically require a large amount of data to learn accurate models, and therefore may be slow to adapt. The payload adaptation task illustrates the need for fast adaptation; the robot must very quickly <strong>determine the payload parameters</strong>, and then <strong>adjust its motor commands accordingly</strong>.</p> <p><br></p> <h3 id="proposed-method-model-based-meta-rl">Proposed method: model-based meta-RL</h3> <p>A <strong>predictive dynamics model</strong> which is <strong>augmented with stochastic latent variables</strong> is learned with different payload masses and tether lengths. In test time, it uses <strong>variational inference to estimate the corresponding posterior distribution over these latent variables</strong>.</p> <p><br> <br> <br></p> <hr> <h1 id="related-work">Related Work</h1> <p><br></p> <p><strong>Controlling Aerial Vehicles</strong></p> <ul> <li>Prior works on control for aerial vehicles that relying on <strong>manual system identification</strong> : require <em>a priori</em> knowledge of the system</li> <li>Automating system identification via <strong>online parameter adaptation</strong> : still rely on domain knowledge for the equations of motion</li> <li>Adaptive model-based controller : require <em>a priori</em> knowledge of dynamic systems, parameters for state estimation, etc.</li> <li>In contrast, the proposed data-driven method only requires <strong>the pixel location of the payload</strong> and <strong>the quadrotor’s commanded actions to control and adapt to the suspended payload’s dynamics</strong>.</li> </ul> <p><br></p> <p><strong>End-to-end Learning-based Approach</strong></p> <ul> <li>The learning processes of value-based and gradient-based methods generally take hours or even days, making it <strong>poorly suited for safety-critical and resource-constrained quadcopters</strong>.</li> <li>Model-based reinforcement learning (MBRL) can provide <strong>better sample efficiency</strong>, but must MBRL methods are <strong>designed to model a single task with unchanging dynamics</strong>, and therefore do not adapt to rapid online changes in the system dynamics.</li> </ul> <p><br></p> <p><strong>Model-based Meta-learning</strong></p> <ul> <li> <a href="https://arxiv.org/abs/2103.01932" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">O'Connell et al.</code></a> used MAML algorithm for adapting a drone’s internal dynamics model. The resulting adapted model, however, <strong>did not improve the performance of the closed-loop controller</strong>. In contrast, the authors demonstrate that their method does improve performance of the model-based controller.</li> <li> <a href="https://arxiv.org/abs/1803.11347" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">Nagabandi et al.(GrBAL)</code></a> and <a href="https://arxiv.org/abs/2003.04663" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">Kaushik et al.</code></a> explored <strong>meta-learning for online adaptation in MBRL</strong> for a legged robot, which demonstrated improved closed-loop controller performance with adapted model.</li> </ul> <p><br> <br> <br></p> <hr> <h1 id="preliminaries-pets-algorithm">Preliminaries: PETS Algorithm</h1> <p><br></p> <p>Model-based reinforcement learning estimates the underlying dynamics from data by training a dynamics model \(p_\theta(s_{t+1} \mid s_t, a_t)\) via maximum likelihood</p> \[\begin{align} \theta^* &amp;= \underset{\theta}{\mathrm{argmax}} \ p(\mathcal D^\text{train} \mid \theta) \nonumber\\ \label{eqn:model-train} &amp;= \underset{\theta}{\mathrm{argmax}} \sum_{(s_t,a_t,s_{t+1}) \in \mathcal D^\text{train}} \mathrm{log} \ p_\theta(s_{t+1} \mid s_t, a_t). \end{align}\] <p>To instantiate this method, the authors extend the <a href="https://arxiv.org/abs/1805.12114" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">PETS algorithm</code></a>, which handles expressive <strong>neural network dynamics models</strong> to attain <strong>good sample efficiency as model-based algorithms</strong> and <strong>asymptotic performance as model-free algorithms</strong>.</p> <p><br> <br></p> <h3 id="probabilistic-ensemble-nn">Probabilistic ensemble NN</h3> <p><br></p> <p><strong><em>Why ‘Probabilistic NN’ and ‘Ensemble of NNs’?</em></strong></p> <ul> <li> <strong>The capacity of model</strong> is a critical ingredient in the asymptotic performance of MBRL methods</li> <li>NN models can scale to large datasets, however, <strong>NNs tend to overfit</strong> on small datasets.</li> <li>This issue can be mitigated by <strong>properly incorporating uncertainty into the dynamics model -&gt; Probailistic NN !</strong> </li> <li>Two types of uncertainty: (1) <em>aleatoric uncertainty</em>: inherent stochasticities of a system, (2) <em>epistemic uncertainty</em>: subjective uncertainty due to a lack of sufficient data to uniquely determine the underlying system exactly.</li> <li> <strong>‘Probabilistic NN’ can capture aleatoric uncertainty</strong> and <strong>‘Ensembles’ can capture epistemic uncertainty</strong>.</li> </ul> <p><br></p> <p>PETS uses an ensemble of probabilistic neural network models, each parameterizing a Gaussian distribution of \(s_{t+1}\) conditioned on both \(s_t\) and \(a_t\) , i.e.: \(\tilde{f} = \mathrm{Pr}(s_{t+1} \mid s_t, a_t) = \mathcal N(\mu_\theta(s_t,a_t), \Sigma_\theta (s_t,a_t))\) . The negative log prediction probability is used for loss function:</p> \[\begin{align*} \mathrm{loss_P}(\theta) &amp;= - \sum_{n=1}^N \mathrm{log}\ \tilde{f}_ \theta (s_{n+1} \mid s_n, a_n) \\ &amp;= \sum_{n=1}^N \left[ \mu_\theta (s_n,a_n) - s_{n+1} \right]^\top \Sigma_\theta^{-1} (s_n,a_n) \left[ \mu_\theta(s_n,a_n) -s_{n+1} \right] + \mathrm{log}\ \mathrm{det}\ \Sigma_\theta(s_n,a_n). \end{align*}\] <p>Ensembles of B-many bootstrap models, using \(\theta_b\) to refer to the parameters of \(b^\mathrm{th}\) model \(\tilde{f}_ {\theta_b}\) , define predictive probability distributions: \(\tilde{f_\theta} = \frac{1}{B} \sum_{b=1}^B \tilde{f}_ {\theta_b}\) . Each of bootstrap models have their unique dataset \(\mathbb D_b\) , generated by sampling (with replacement) \(N\) times the dynamics dataset recorded so far \(\mathbb D\) , where \(N\) is the size of \(\mathbb D\) . A visual example of ensembles is provided below.</p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MBMRL-Flight/PETS-model-ensembles.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MBMRL-Flight/PETS-model-ensembles.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MBMRL-Flight/PETS-model-ensembles.PNG-1400.webp"></source> <img src="/assets/img/MBMRL-Flight/PETS-model-ensembles.PNG" class="img-fluid" width="auto" height="auto" title="Example of Probabilistic Model Ensembles" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models" </div> <p><br> <br></p> <h3 id="state-propagation-via-trajectory-sampling-ts">State Propagation via Trajectory Sampling (TS)</h3> <p>The learned dynamics model is used to plan and execute actions via model predictive control (MPC) with trajectory sampling (TS), since the <strong><em>probabilistic</em> dynamics model \(\tilde{f}\) induces a distribution over the resulting trajectories \(s_{t:t+T}\)</strong> for a given control input \(a_{t:t+T} \doteq \lbrace a_t, \ldots , a_{t+T} \rbrace\) . Trajectory sampling predicts plausible state trajectories begins by creating \(P\) particles from the current state, \(s_{t=0}^p =s_0 \ \forall p\) . Each particle is then propagated by: \(s_{t+1}^p \sim \tilde{f}_ {\theta_{b(p,t)}} (s_t^p,a_t)\) , according to a particular bootstrap \(b(p,t)\ \text{in} \lbrace 1, \ldots, B \rbrace\) , where \(B\) is the number of bootstrap models in the ensemble.</p> <p><br> <br></p> <h3 id="optimizing-action-sequence-via-cross-entropy-method-cem">Optimizing action sequence via Cross-Entropy Method (CEM)</h3> <p>Unlike a common technique to compute the optimal action sequence (random sampling shooting), PETS used <a href="https://www.researchgate.net/publication/279242256_Chapter_3_The_Cross-Entropy_Method_for_Optimization" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">cross-entropy method(CEM)</code></a>. The <em>cross-entropy (CE) method</em> is an <strong>adaptive importance sampling procedure for the estimation of rare-event probabilities</strong>, which uses the <em>cross-entropy</em> or <em>Kullback-Leibler divergence</em> as a measure of closeness between two sampling distributions. By using the cross-entropy method to <strong>gradually change the sampling distribution</strong> of the random search so that <strong>the rare-event</strong>, especially locating an optimal or near optimal solution using naive random search, <strong>is more likely to occur</strong>. Eventually, the sampling distribution converges to a distribution with a probability mass concentrated in a region of near-optimal solutions.</p> <p>The CE method can be applied to two types of problems:</p> <ul> <li> <strong>Estimation</strong>: Estimate \(l = \mathbb E[H(\mathbf X)]\) , where \(\mathbf X\) is a random object taking values in some set \(\mathfrak X\) and \(H\) is a function on \(\mathfrak X\) . An important special case is the estimation of a probability \(l = \mathbb P(S(\mathbf X) \geqslant \gamma)\) , where \(S\) is another function on \(\mathfrak X\) .</li> <li> <strong>Optimization</strong>: Optimize a given objective function \(S(\mathbf x)\) over all \(\mathbf x \in \mathfrak X\) . \(S\) can be either a known or a <em>noisy</em> function. In the latter case the objective function needs to be estimated, e.g. via simulation.</li> </ul> <p><br></p> <p><strong>Cross-Entropy for Rare-event Probability Estimation</strong></p> <p>Consider the estimation of the probability</p> \[l = \mathbb P(S(\mathbf X) \geqslant \gamma) = \mathbb E \left[ \mathbf I_{\lbrace S(\mathbf X) \geqslant \gamma \rbrace} \right] = \int \mathbf I_{\lbrace S(\mathbf X) \geqslant \gamma \rbrace} f(\mathbf x ; \mathbf u) \, d\mathbf x ,\] <p>where \(S\) : real-valued function, \(\gamma\) : threshold or level parameter, \(\mathbf X \sim f(\cdot;\mathbf u)\) : pdf parameterized by a finite-dim vector \(\mathbf u\)</p> <p>Let \(g\) be another pdf s.t. \(g(\mathbf x) = 0 \Rightarrow H(\mathbf x)f(\mathbf x; \mathbf u) = 0 \ \forall \mathbf x\) . Using the pdf \(g\) , we can represent \(l\) as</p> \[l = \int \frac{f(\mathbf x; \mathbf u) \mathbf I_{\lbrace S(\mathbf x) \geqslant \gamma \rbrace}}{g(\mathbf x)} g(\mathbf x) \, d \mathbf x = \mathbb E \left[ \frac{f(\mathbf X; \mathbf u) \mathbf I_{\lbrace S(\mathbf X) \geqslant \gamma \rbrace}}{g(\mathbf X)} \right] , \quad \mathbf X \sim g .\] <p>With independent random vectors \(\mathbf{X_1, \ldots, X_N} \underset{\mathrm{iid}}{\sim} g\) , then</p> \[\hat{l} = \frac{1}{N} \sum_{k=1}^N \mathbf I_{\lbrace S(\mathbf X_k) \geqslant \gamma \rbrace} \frac{f(\mathbf X_k;\mathbf u)}{g(\mathbf X_k)}\] <p>is an unbiased estimator of \(l\) : a so-called <em>importance sampling estimator</em>. The optimal importance sampling pdf (that is, the pdf \(g^*\) for which the variance of \(\hat{l}\) is minimal is given as \(g^* (\mathbf x) = f(\mathbf x; \mathbf u) \mathbf I_{\lbrace S(\mathbf x) \geqslant \gamma \rbrace} / l\) . However, <strong>as \(l\) is unknown</strong>, CE method choose the importance sampling pdf \(g\) from within the parametric class of pdfs \(\lbrace f(\cdot;\mathbf v), \mathbf v \in \mathcal V \rbrace\) s.t. the KL divergence between \(g^*\) and \(g\) is minimal. The CE minimization procedure then reduces to finding an optimal reference parameter vector, \(\mathbf v^*\) say, by cross-entropy minimization:</p> \[\begin{align*} \mathbf v^* &amp;= \underset{\mathbf v}{\mathrm{argmin}} \mathcal D(g^* , f(\cdot; \mathbf v)) \\ &amp;= \underset{\mathbf v}{\mathrm{argmin}} \int g^* (\mathbf x) \mathrm{ln}\ \frac{g^* (\mathbf x)}{f(\mathbf x;\mathbf v)} \, d \mathbf x \\ &amp;= \underset{\mathbf v}{\mathrm{argmax}} \int g^* (\mathbf x) \mathrm{ln}\ f(\mathbf x;\mathbf v) \, d \mathbf x \\ &amp;= \underset{\mathbf v}{\mathrm{argmax}} \int \frac{f(\mathbf x; \mathbf u) \mathbf I_{\lbrace S(\mathbf x) \geqslant \gamma \rbrace}}{l} \mathrm{ln}\ f(\mathbf x;\mathbf v) \, d \mathbf x \\ &amp;= \underset{\mathbf v}{\mathrm{argmax}} \mathbb E_{\mathbf u} \left[ \mathbf I_{\lbrace S(\mathbf X) \geqslant \gamma \rbrace} \mathrm{ln}\ f(\mathbf X;\mathbf v) \right] \\ &amp;= \underset{\mathbf v}{\mathrm{argmax}} \mathbb E_{\mathbf w} \left[ \mathbf I_{\lbrace S(\mathbf X) \geqslant \gamma \rbrace} \mathrm{ln}\ f(\mathbf X;\mathbf v) \frac{f(\mathbf X;\mathbf u)}{f(\mathbf X;\mathbf w)} \right], \end{align*}\] <p>where \(\mathbf w\) is any reference parameter. This \(\mathbf v^*\) can be estimated via the stochastic sampling:</p> \[\hat{\mathbf v} = \underset{\mathbf v}{\mathrm{argmax}} = \frac{1}{N} \sum_{k=1}^N \mathbf I_{\lbrace S(\mathbf X_k) \geqslant \gamma \rbrace} \frac{f(\mathbf X_k;\mathbf u)}{f(\mathbf X_k;\mathbf w)} \mathrm{ln}\ f(\mathbf X_k;\mathbf v),\] <p>where \(\mathbf{X_1, \ldots, X_N} \underset{\mathrm{iid}}{\sim} f(\cdot;\mathbf w)\) . The optimal parameter \(\hat{\mathbf v}\) can often be obtained in explicit form, in particular when the class of sampling distributions forms an <em>exponential family</em>.</p> <p><br></p> <p>For a rare-event probability \(l\) , most or all of the indicators \(\mathbf I_{\lbrace S(\mathbf X) \geqslant \gamma \rbrace}\) are zero, and the maximization problem become useless. In that case a <strong>multi-level CE</strong> procedure is used, where a sequence of reference parameters \(\lbrace \hat{\mathbf v}_ t \rbrace\) and levels \(\lbrace \hat{\gamma}_ t \rbrace\) is constructed with the goal that the former converges to \(\mathbf v^*\) and the latter to \(\gamma\) . The actual procedure is described in the following algorithm.</p> <p><strong>Algorithm A (CE Algorithm for Rare-Event Estimation)</strong> <em>Given the sample size \(N\) and the rarity parameter \(\varrho\) , execute the following steps.</em></p> <ol> <li><em>Define \(\hat{\mathbf v}_ 0 = \mathbf u\) . Let \(N^e = \left \lceil \varrho N \right \rceil\) . Set \(t=1\) (iteration counter).</em></li> <li><em>Generate \(\mathbf{X_1, \ldots, X_N} \underset{\mathrm{iid}}{\sim} f(\cdot;\hat{\mathbf v}_ {t-1})\) . Calculate \(S_{(i)} = S(\mathbf X_i)\ \forall i\) , and order these from smallest to largest: \(S_{(1)} \leqslant \ldots \leqslant S_{(N)}\) . Let \(\hat{\gamma}_ t\) be the sample \((1-\varrho) \text{-quantile of performances}\) ; that is, \(\hat{\gamma}_ t = S_{(N-N^e+1)}\) . If \(\hat{\gamma}_ t &gt; \gamma\) , reset \(\hat{\gamma}_ t\) to \(\gamma\) .</em></li> <li> <em>Use the</em> <strong>same</strong> <em>sample \(\mathbf{X_1, \ldots, X_N}\) to solve the stochastic program</em>: \(\hat{\mathbf v}_ t = \underset{\mathbf v}{\mathrm{argmax}} = \frac{1}{N} \sum_{k=1}^N \mathbf I_{\lbrace S(\mathbf X_k) \geqslant \hat{\gamma}_ t \rbrace} \frac{f(\mathbf X_k;\mathbf u)}{f(\mathbf X_k;\hat{\mathbf v}_ {t-1})} \mathrm{ln}\ f(\mathbf X_k;\mathbf v).\)</li> <li><em>If \(\hat{\gamma}_ t &lt; \gamma\) , set \(t = t+1\) and reiterate from Step 2; otherwise, proceed with Step 5.</em></li> <li> <em>Let \(T=t\) be the final iteration counter. Generate \(\mathbf{X_1, \ldots, X_N} \underset{\mathrm{iid}}{\sim} f(\cdot;\hat{\mathbf v}_ T)\) and estimate \(l\) via importance sampling</em>:</li> </ol> \[\hat{l} = \frac{1}{N} \sum_{k=1}^N \mathbf I_{\lbrace S(\mathbf X_k) \geqslant \gamma \rbrace} \frac{f(\mathbf X_k;\mathbf u)}{f(\mathbf X_k;\hat{\mathbf v}_ T)}.\] <p><br></p> <p><strong>Cross-Entropy Method for Optimization</strong></p> <p>The estimation algorithm above leads naturally to a simple optimization heuristic. Optimization problem can be written, with an assumtion that only one maximizer \(\mathbf x^*\) exists for simplicity, as</p> \[S(\mathbf x^* ) = \gamma^* = \underset{\mathbf x \in \mathfrak X}{\mathrm{max}}\ S(\mathbf x).\] <p>We can now associate with the above optimization problem the estimation of the probability \(l = \mathbb P(S(\mathbf X) \geqslant \gamma)\) , where \(\gamma\) is close to the unknown \(\gamma^*\) . Typically, \(l\) is a rare-event probability, and the multi-level CE approach of Algorithm A can be used to find <strong>an importance sampling distribution that concentrates all its mass in a neighborhood of the point \(\mathbf x^*\)</strong> . Sampling from such a distribution thus produces optimal or near-optimal states. Although the final level \(\gamma = \gamma^*\) is generally not known in advance, the CE method for optimization produces a sequence of levels \(\lbrace \hat{\gamma}_ t \rbrace\) and reference parameters \(\lbrace \hat{\mathbf v}_ t \rbrace\) such that ideally the former tends to the optimal \(\gamma^*\) and the latter to the optimal reference vector \(\mathbf v^*\) corresponding to the point mass at \(\mathbf x^*\) .</p> <p><strong>Algorithm B (CE Algorithm for Optimization)</strong></p> <ol> <li><em>Choose an initial parameter vector \(\hat{\mathbf v}_ 0\) . Let \(N^e = \left \lceil \varrho N \right \rceil\) . Set \(t=1\) (level counter).</em></li> <li><em>Generate \(\mathbf{X_1, \ldots, X_N} \underset{\mathrm{iid}}{\sim} f(\cdot;\hat{\mathbf v}_ {t-1})\) . Calculate the performances \(S_{(i)} = S(\mathbf X_i) \forall i\) , and order them from smallest to largest: \(S_{(1)} \leqslant \ldots \leqslant S_{(N)}\) . Let \(\hat{\gamma}_ t\) be the sample \((1-\varrho) \text{-quantile of performances}\) ; that is, \(\hat{\gamma}_ t = S_{(N-N^e+1)}\) .</em></li> <li> <em>Use the</em> <strong>same</strong> <em>sample \(\mathbf{X_1, \ldots, X_N}\) to solve the stochastic program</em>: \(\begin{equation} \hat{\mathbf v}_ t = \underset{\mathbf v}{\mathrm{argmax}} = \frac{1}{N} \sum_{k=1}^N \mathbf I_{\lbrace S(\mathbf X_k) \geqslant \hat{\gamma}_ t \rbrace} \mathrm{ln}\ f(\mathbf X_k;\mathbf v). \end{equation}\)</li> <li><em>If some stopping criterion is met, stop; otherwise set \(t=t+1\) , and return to Step 2.</em></li> </ol> <p>Note that the estimation Step 5 of Algorithm A is missing in Algorithm B, because in the optimization setting we are not interested in estimating \(l\) per se. For the same reason the likelihood ration term \(f(\mathbf X_k;\mathbf u) / f(\mathbf X_k;\hat{\mathbf v}_ {t-1})\) in Algorithm A is missing in Algorithm B. To run the algorithm, (1) a class of parametric sampling densities \(\lbrace f(\cdot;\mathbf v), \ \mathbf v \in \mathcal V \rbrace\) , (2) the initial vector \(\hat{\mathbf v}_ 0\) , (3) the sample size \(N\) , (4) the rarity parameter \(\varrho\) , and (5) a stopping criterion are needed to be predefined.</p> <p><br></p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MBMRL-Flight/PETS-CEM-performance-comparison.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MBMRL-Flight/PETS-CEM-performance-comparison.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MBMRL-Flight/PETS-CEM-performance-comparison.PNG-1400.webp"></source> <img src="/assets/img/MBMRL-Flight/PETS-CEM-performance-comparison.PNG" class="img-fluid" width="auto" height="auto" title="MPC action optimizer comparison" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models" </div> <p>The results from <a href="https://arxiv.org/abs/1805.12114" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">PETS paper</code></a> show that using CEM significantly outperforms random search on the half-cheetah task. Simple random search techniques are simple and have ease of parallelism, but they suffer in high dimensional spaces.</p> <p><br> <br></p> <h3 id="pets-algorithm-summary">PETS algorithm summary</h3> <p>Now the overall PETS algorithm can be summarized in Algorithm 1.</p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MBMRL-Flight/PETS-algorithm.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MBMRL-Flight/PETS-algorithm.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MBMRL-Flight/PETS-algorithm.PNG-1400.webp"></source> <img src="/assets/img/MBMRL-Flight/PETS-algorithm.PNG" class="img-fluid" width="auto" height="auto" title="PETS algorithm" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models" </div> <p><br> <br> <br></p> <hr> <h1 id="model-based-meta-learning-for-quadcopter-payload-transport">Model-Based Meta-Learning For Quadcopter Payload Transport</h1> <p><br></p> <div class="row justify-content-center"> <div class="col-10"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MBMRL-Flight/MBMRL-Flight-SystemDiagram.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MBMRL-Flight/MBMRL-Flight-SystemDiagram.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MBMRL-Flight/MBMRL-Flight-SystemDiagram.PNG-1400.webp"></source> <img src="/assets/img/MBMRL-Flight/MBMRL-Flight-SystemDiagram.PNG" class="img-fluid" width="auto" height="auto" title="System diagram of MBMRL algorithm for flight" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads" </div> <p><br></p> <h3 id="data-collection">Data Collection</h3> <p>Data is collected by manually piloting the quadcopter along random paths for each of the \(K\) suspended payloads. A dataset \(\mathcal D^\text{train}\) consists of \(K\) separate datasets \(\mathcal D^\text{train} \doteq \mathcal D^\text{train}_ {1:K} \doteq \lbrace \mathcal D^\text{train}_ 1, \ldots , \mathcal D^\text{train}_ K \rbrace\) , one per payload task.</p> <p><br> <br></p> <h3 id="model-training-with-known-dynamics-variables">Model Training with Known Dynamics Variables</h3> <p>In case we know all the “factors of variation” in the dynamics across tasks at training time, represented explicitly as a “dynamic variable” \(z_k \in \mathbb R^{d_z}\) , we can learn a single dynamics model \(p_\theta\) across all tasks by using \(z_k\) as an auxiliary input to PETS:</p> \[\begin{equation} s_{t+1} \sim p_\theta (s_{t+1} \mid s_t, a_t, z_k ). \end{equation}\] <p>Training is analogous to \eqref{eqn:model-train} , but with an additional conditioning on \(z_{1:K} \doteq [ z_1, \ldots, z_K ]\) :</p> \[\begin{align} \theta^* &amp;= \underset{\theta}{\mathrm{argmax}} \ p(\mathcal D^\text{train} \mid z_{1:K}, \theta) \nonumber\\ \label{eqn:known-model-train} &amp;= \underset{\theta}{\mathrm{argmax}} \sum_{k=1}^K \sum_{(s_t,a_t,s_{t+1}) \in \mathcal D^\text{train}_ k} \mathrm{log} \ p_\theta(s_{t+1} \mid s_t, a_t, z_k). \end{align}\] <p><br> <br></p> <h3 id="meta-training-with-latent-dynamics-variables">Meta-Training with Latent Dynamics Variables</h3> <p>In most cases, we can’t know or measure dynamic factors at every training time. Thus, a more general training procedure that <strong><em>infers</em> the dynamics variables \(z_{1:K}\) and the model parameters \(\theta\) <em>jointly</em></strong>. This is begun by placing a prior over \(z_{1:K} \sim p(z_{1:K}) = \mathcal N(0,I)\) , and then jointly infer the posterior \(p(\theta, z_{1:K} \mid \mathcal D^\text{train}_ {1:K} )\) .</p> <p>Unfortunately, inferring \(p(\theta, z_{1:K} \mid \mathcal D^\text{train}_ {1:K} )\) exactly is computationally intractable. Therefore, the authors used an <strong>approximate variational posterior</strong>, which is a Gaussian with diagonal covariance, factored over tasks,</p> \[\begin{equation} q_{\phi_k} (z_k) = \mathcal N(\mu_k, \Sigma_k) \approx p(z_k \mid \mathcal D^\text{train}) \quad \forall k \in [K], \end{equation}\] <p>and parameterized by \(\phi_k \doteq \lbrace \mu_k, \Sigma_k \rbrace\) . Unlike the case of known dynamics variables \eqref{eqn:known-model-train}, now we must marginalize out \(z_{1:K}\) because it is unknown. Therfore, the model training with latent dynamics variables can be written as:</p> \[\begin{align} \mathrm{log}\ p(\mathcal D^\text{train} &amp; \mid \theta) = \mathrm{log} \int_{z_{1:K}} p(\mathcal D^\text{train} \mid z_{1:K}, \theta) p(z_{1:K})\, dz_{1:K} \nonumber\\ &amp;= \sum_{k=1}^K \mathrm{log}\ \mathbb E_{z_k \sim q_{\phi_k}} p(\mathcal D^\text{train} \mid z_{1:K}, \theta) \cdot p(z_{1:K})/q_{\phi_k}(z_k) \nonumber\\ &amp;\geq \sum_{k=1}^K \mathbb E_{z_k \sim q_{\phi_k}} \sum_{(s_t,a_t,s_{t+1}) \in \mathcal D^\text{train}_ k} \mathrm{log}\ p_\theta(s_{t+1} \mid s_t, a_t, z_k) - \mathrm{KL}(q_{\phi_k}(z_k) \parallel p(z_k)) \quad (\because \text{def. of KL-div &amp; } 0 \leq q \leq 1 \to \text{inequality holds)} \nonumber\\ \label{eqn:marginalize-latent} &amp;\doteq \mathrm{ELBO}(\mathcal D^\text{train} \mid \theta, \phi_{1:K}) . \end{align}\] <p>The propsoed meta-training algorithm then optimizes both \(\theta\) and the variational parameters \(\phi_{1:K}\) of each task with respect to the evidence lower bound</p> \[\begin{equation}\label{eqn:unknown-model-train} \theta^* \doteq \underset{\theta}{\mathrm{argmax}}\ \underset{\phi_{1:K}}{\mathrm{max}}\ \mathrm{ELBO}(\mathcal D^\text{train} \mid \theta, \phi_{1:K}). \end{equation}\] <p><br> <br></p> <h3 id="test-time-task-inference">Test-Time Task Inference</h3> <p>At test time, the robot must infer the unknown dynamics variables \(z^\text{test}\) online in order to improve the learned dynamics model \(p_{\theta*}\) and the resulting MPC planner. Similarly to meta-training with latent variables, a variational approximation is used for \(z^\text{test}\) :</p> \[\begin{equation} q_{\phi^\text{test}}(z^\text{test}) = \mathcal N(\mu^\text{test}, \Sigma^\text{test}) \approx p(z^\text{test} \mid \mathcal D^\text{test}), \end{equation}\] <p>parameterized by \(\phi^\text{test} \doteq \lbrace \mu^\text{test}, \Sigma^\text{test} \rbrace\) . Variational inference is used to optimze \(\phi^\text{text}\) such that <strong>the approximate distribution \(q_{\phi^\text{test}}(z^\text{test})\) is close to the true distribution \(p(z^\text{test} \mid \mathcal D^\text{test}\)</strong> , measured by the Kullback-Leibler divergence:</p> \[\begin{align} \phi^* &amp;\doteq \underset{\phi}{\mathrm{argmax}}\ -\mathrm{KL}( q_\phi(z^\text{test} \parallel p(z^\text{test} \mid \mathcal D^\text{test}, \theta*)) \nonumber\\ &amp;= \underset{\phi}{\mathrm{argmax}}\ \mathbb E_{z^\text{test} \sim q_\phi} \mathrm{log}\ p(z^\text{test} \mid \mathcal D^\text{test}, \theta*) - \mathrm{log}\ q_\phi(z^\text{test}) \nonumber\\ &amp;= \underset{\phi}{\mathrm{argmax}}\ \mathbb E_{z^\text{test} \sim q_\phi} \mathrm{log}\ p(z^\text{test} \mid \mathcal D^\text{test}, \theta*) - \mathrm{log}\ q_\phi(z^\text{test}) + \mathrm{log}\ p(z^\text{test}) \nonumber\\ &amp;= \underset{\phi}{\mathrm{argmax}}\ \mathbb E_{z^\text{test} \sim q_\phi} \sum_{(s_t,a_t,s_{t+1}) \in \mathcal D^\text{test}} \mathrm{log}\ p_{\theta*}(s_{t+1} \mid s_t, a_t, z^\text{test}) - \mathrm{KL}(q_\phi(z^\text{test} \parallel p(z^\text{test})) \nonumber\\ \label{eqn:test-objective} &amp;= \underset{\phi}{\mathrm{argmax}}\ \mathrm{ELBO}(\mathcal D^\text{test} \mid \theta^* , \phi). \end{align}\] <p>Note the objective \eqref{eqn:test-objective} corresponds to the test-time ELBO of \(\mathcal D^\text{test}\) , analogous to training-time ELBO of \(\mathcal D^\text{train}\) \eqref{eqn:marginalize-latent}. As equation \eqref{eqn:test-objective} is tractable to optimize, and therefore at test time we perform <strong>gradient descent online</strong> in order to learn \(\phi^\text{test}\) and therefore improve the predictions of our learned dynamics model.</p> <p>The overall training and test time graphical models are summarized in Figure 4.</p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MBMRL-Flight/meta-rl-graphical-model.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MBMRL-Flight/meta-rl-graphical-model.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MBMRL-Flight/meta-rl-graphical-model.PNG-1400.webp"></source> <img src="/assets/img/MBMRL-Flight/meta-rl-graphical-model.PNG" class="img-fluid" width="auto" height="auto" title="Probabilistic graphical models of the drone-payload system dynamics" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads" </div> <p><br> <br></p> <h3 id="method-summary">Method Summary</h3> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MBMRL-Flight/MBMRL-Flight-algorithm.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MBMRL-Flight/MBMRL-Flight-algorithm.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MBMRL-Flight/MBMRL-Flight-algorithm.PNG-1400.webp"></source> <img src="/assets/img/MBMRL-Flight/MBMRL-Flight-algorithm.PNG" class="img-fluid" width="auto" height="auto" title="Algorithm of MBMRL for Quadcopter Payload Transport" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads" </div> <p><br></p> <h3 id="method-implementation">Method Implementation</h3> <p><br></p> <p><strong>Payload Variations</strong></p> <ul> <li>3D printed payloads weighing between 10-15 grams.</li> <li>Experiments <strong>vary primarily the string length</strong> between 18-30cm long (18cm or 30cm). (since the dynamics are more sensitive to string length than mass)</li> </ul> <p><strong>Data Collection Spec.</strong></p> <ul> <li>actions \(a \in \mathbb R^3\) : Cartesian velocity commands</li> <li>states \(s \in \mathbb R^3\) : pixel location \(\mathbb R^2\) X size of the payload \(\mathbb R\)</li> </ul> <p><strong>Dynamic Model \(p_\theta\) and MPC Details</strong></p> <ul> <li>NN consists of four FC hidden layers of size 200 with <em>swish activations</em> (also known as SeLU).</li> <li>MPC is run with a time horizon of 5 steps, using the <strong>cross entropy method</strong> to optimize, with a sample size 50, selecting 10 elite samples and 3 iterations.</li> <li>MPC computation takes 50-100ms -&gt; select control frequency to be 4Hz for both training and test time -&gt; 150-200ms for latent variable inference</li> </ul> <p><br> <br> <br></p> <hr> <h1 id="experimental-evaluation">Experimental Evaluation</h1> <p><br></p> <p><strong>Aims of Experiments</strong></p> <ul> <li> <strong>Q1</strong> Does <strong>online adaptation</strong> via meta-learning lead to <strong>better performance compared to non-adaptive methods?</strong> </li> <li> <strong>Q2</strong> How does our <strong>meta-learning approach</strong> compare to <strong>MBRL conditioned on a history</strong> of states and actions?</li> <li> <strong>Q3</strong> How does our approach with <strong>known versus unknown dynamics variables</strong> compare?</li> <li> <strong>Q4</strong> <strong>Can we <em>generalize</em> to payloads</strong> that were not seen at training time?</li> <li> <strong>Q5</strong> Is the test-time inference procedure able to <strong>differentiate between different <em>a priori</em> unknown payloads?</strong> </li> <li> <strong>Q6</strong> Can our approach enable a quadcopter to fulfill a complete payload pick-up, transport, and drop-off task, as well as other <strong>realistic payload transportation scenarios?</strong> </li> </ul> <p><br></p> <p><strong>Baseline Approaches</strong></p> <ul> <li> <strong><em>MBRL</em></strong>: state consists of only the current payload pixel location and size</li> <li> <strong><em>MBRL with history</em></strong>: simple meta-learning approach, where the state consists of the past 8 states and actions concatenated together</li> <li> <strong><em>PID controller</em></strong>: three PID controllers for each Cartesian velocity command axis. PID gains are manually tuned by evaluating the performance of the controller on a trajectory following path not used in this experiments for a single payload.</li> </ul> <p><br> <br></p> <h3 id="trajectory-following">Trajectory Following</h3> <ul> <li>Tracking specified payload trajectories: a <strong>circle</strong> or <strong>square path</strong> in the image plane or a <strong>figure 8 path</strong> parallel to the ground (with a suspended cable either 18cm or 30cm long)</li> <li>Used a <strong>latent variable of dimension one</strong> </li> </ul> <p><br></p> <div class="row justify-content-center"> <div class="col-8"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking-result.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking-result.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking-result.PNG-1400.webp"></source> <img src="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking-result.PNG" class="img-fluid" width="auto" height="auto" title="Results of trajectory tracking experiments" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking.PNG-1400.webp"></source> <img src="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking.PNG" class="img-fluid" width="auto" height="auto" title="Visualizations of trajectory tracking experiments" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads" </div> <ul> <li>Both the online adaptation methods(=the proposed method and MBRL with history) better track the specified goal trajectories compared to the non-adaptation methods(=MBRL and PID controller)<strong>(Q1)</strong>.</li> <li>The proposed method outperforms the other meta-learning method MBRL with history <strong>(Q2)</strong>.</li> <li>The proposed method <strong>with unknown latent variables at training time outperforms</strong> the proposed method <strong>with known latent variables (Q3)</strong>. Inferring unknown latent variables at training-time might <strong>captures unspecified types of variation from potentially hard to observe factors</strong>.</li> <li>The proposed method has an ability to <strong>generalize to new payloads not seen during training (Q4)</strong>. Learning how string lengths affect the dynamcis benefited from a few minutes of data from a third (24cm) string length, allowing the algorithm to rapidly interpolate to unseen string lengths of 21cm or 27cm at test-time, shown in Table 2.</li> </ul> <p><br></p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking-graph.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking-graph.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking-graph.PNG-1400.webp"></source> <img src="/assets/img/MBMRL-Flight/MBMRL-trajectory-tracking-graph.PNG" class="img-fluid" width="auto" height="auto" title="trajectory tracking latent variables" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads" </div> <ul> <li> <strong>The dynamic variable converges to different values depending on the cable length</strong>, which shows that <strong>the test-time inference procedure is able to differentiate between the dynamics of the two different payloads (Q5)</strong>.</li> <li>As the <strong>inferred value converges</strong>, <strong>the learned model-based controller becomes more accurate</strong> and is therefore better able to track the desired path <strong>(Q1)</strong>.</li> </ul> <p><br> <br></p> <h3 id="end-to-end-payload-transportation">End-to-End Payload Transportation</h3> <p><br></p> <div class="row justify-content-center"> <div class="col-10"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MBMRL-Flight/MBMRL-payload-transportation-result.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MBMRL-Flight/MBMRL-payload-transportation-result.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MBMRL-Flight/MBMRL-payload-transportation-result.PNG-1400.webp"></source> <img src="/assets/img/MBMRL-Flight/MBMRL-payload-transportation-result.PNG" class="img-fluid" width="auto" height="auto" title="Payload transportation task results" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads" </div> <ul> <li>The proposed method successfully completes the full task <strong>(Q6)</strong> due to the online adaptation mechanism <strong>(Q1, Q5)</strong>.</li> <li>Each time the quadcopter transitions between transporting a payload and not transporing a payload, the quadcopter <strong>re-adapt online</strong> to be able to successfully follow the specified trajectories.</li> </ul> <p><br> <br></p> <h3 id="additional-use-cases">Additional Use Cases</h3> <ul> <li>Additional payload transportation cases are available on <a href="https://sites.google.com/view/ral-meta-rl-for-flight" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">the authors' website</code></a> </li> <li>The proposed method is able to transport a suspended payload <strong>(Q6): towards a moving target, around an obstacle by following a predifined path,</strong> and <strong>along trajectories dictated using a “wand”-like interface</strong>.</li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Wooyeol Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script async src="https://cdn.panelbear.com/analytics.js?site="></script> <script>window.panelbear=window.panelbear||function(){(window.panelbear.q=window.panelbear.q||[]).push(arguments)},panelbear("config",{site:""});</script> </body> </html>