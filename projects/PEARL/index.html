<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>PEARL | Wooyeol Lee</title> <meta name="author" content="Wooyeol Lee"> <meta name="description" content="Review on " efficient off-policy meta-reinforcement learning via probabilistic context variables> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, RL, Reinforcement, Learning, AI, Robot, work-out, coffee, travel"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9C%88%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://thisiswooyeol.github.io/projects/PEARL/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Wooyeol Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">PEARL</h1> <p class="post-description">Review on "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables"</p> </header> <article> <h1 id="tldr">TL;DR:</h1> <ul> <li>PEARL decouples the problems of <strong>inferring the task</strong> and <strong>solving it</strong>, allowing for <strong>off-policy meta-learning</strong> while minimizing mismatch between train and test context distributions.</li> <li>With <strong>posterior sampling on latent context variable</strong> on which the policy is conditioned, PEARL can conduct <strong>temporally extended exploration</strong> to adapt unseen tasks rapidly.</li> <li> <strong>Permutation-invariant encoder</strong> for the latent context variable allows decomposing a trajectory into a collection of single transitions.</li> <li>To train context variable in a on-policy manner, context sampler \(\mathcal S_c\) <strong>samples recently collected data</strong> retaining on-policy performance with better efficiency.</li> <li>PEARL outperforms prior algorithms in <strong>sample efficiency by 20-100X</strong> as well as in <strong>asymptotic performance by 50-100%</strong> in five of six domains.</li> </ul> <p><br> <br> <br></p> <hr> <h1 id="introduction">Introduction</h1> <p><br></p> <h3 id="meta-rl-problems">Meta-RL problems</h3> <p>Modern meta-learning is predicated on the assumption that <strong>the distribution of data used for adaptation will match across meta-testing and meta-test</strong>. As on-policy data will be used to adapt at meta-test time, on-policy data should be used during meta-training time as well. This makes algorithms exceedingly inefficient during meta-training and makes them inherently difficult to meta-train a policy to adapt using off-policy data.</p> <p><br> <br></p> <h3 id="how-to-tackle-the-problem-">How to tackle the problem ?</h3> <p>To achieve both meta-training efficiency(off-policy) and rapid adaptation(task inference), they integrate <strong>online inference of probabilistic context variables with existing off-policy RL algorithms</strong>. During meta-training, PEARL learns a probabilistic encoder that <strong>accumulates the necessary statistics from past experience into the context variables</strong>. At meta-test time, trajectories are collected with sampled context variables(=”task hypotheses”) and the collected trajectories update the posterior over the context variables, achieveing rapid trajectory-level adaptation.</p> <p><strong>Disentangling task inference from action</strong> makes PEARL particularly amenable to off-policy meta-learning; the policy can be optimized with off-policy data while the probabilistic encoder is trained with on-policy data to minimize distribution mismatch between meta-train and meta-test.</p> <p><br> <br></p> <h3 id="experiments-summary">Experiments summary</h3> <p>Probabilistic embeddings for actor-critic RL (PEARL) achieves state-of-the-art results with 20-100X improvement in meta-training sample efficiency and substantial increases in asymptotic performance over prior state-of-the-art on six continuous control meta-learning environments. They further examine how PEARL conducts <strong>structured exploration</strong> to adapt rapidly to new tasks in a 2-D navigation environments with sparse rewards.</p> <p><br> <br> <br></p> <hr> <h1 id="related-work">Related Work</h1> <p><br></p> <h3 id="meta-learning">Meta-learning</h3> <p><strong><em>Context-based</em> meta-RL</strong></p> <ul> <li>Prior works with recurrent and recursive meta-RL method adapt to new tasks by aggregating experience into a latent representation on which the policy is conditioned. (whole experience = task specific context)</li> <li> <em>However</em>, PEARL represents task contexts with <strong>probabilistic latent variables</strong>, enabling reasoning over task uncertainty.</li> <li>Instead of using recurrence, PEARL leverages the Markov property in the <strong>permutation-invariant encoder</strong> to aggregate experience, enabling fast optimization especially for long-horizon tasks while mitigating overfitting.</li> </ul> <p><strong><em>Gradient-based</em> meta-RL</strong></p> <ul> <li>Gradient-based meta-RL methods focus on on-policy meta-learning as off-policy data is non-trivial to do with policy gradient methods.</li> <li>They found that PEARL (context-based method) is able to reach higher asymptotic performance, in comparison to methods using policy gradients.</li> </ul> <p><strong>Motivation of permutation-invariant encoder</strong></p> <ul> <li>Permutation-invariant embedding function described in this paper is inspired by the embedding function of prototypical networks.</li> <li>While they use a distance metric in a learned, <em>deterministic</em> embedding space to classify new inputs, embeddings in this paper is <em>probabilistic</em> and is used to condition the behavior of an RL agent.</li> </ul> <p><br> <br></p> <h3 id="probabilistic-meta-learning">Probabilistic meta-learning</h3> <ul> <li>Probabilistic latent task variables: used to adapt model predictions for supervised learning &lt;- <em>extend this idea to off-policy meta-RL !</em> </li> <li>PEARL infers task variables and explores via <strong>posterior sampling</strong>.</li> </ul> <p><br> <br></p> <h3 id="posterior-sampling">Posterior sampling</h3> <ul> <li>In classical RL, posterior sampling maintains a posterior over possible MDPs and enables <strong>temporally extended exploration</strong> by acting optimally according to a sampled MDP.</li> <li>PEARL: meta-learned variant of this method; probabilistic context captures the <strong>current uncertainty over the task</strong>, allowing the agent to <strong>explore in new tasks</strong> in a similarly structured manner.</li> </ul> <p><br> <br></p> <h3 id="partially-observed-mdps">Partially observed MDPs</h3> <ul> <li>Adaptation at test time in meta-RL can be viewed as a special case of RL in a POMDP:</li> </ul> \[\tilde{\mathcal M} = \left\lbrace \mathcal{\tilde{S}, A, \tilde{O}, \tilde{P}, E, r} \right\rbrace \quad \text{where} \quad \mathcal{\tilde{S} = S \times Z}, \tilde{s} = (s,z) \ \text{and} \ \mathcal{\tilde{O} = S}, \tilde o = s\] <ul> <li>The explicit state estimation \(p(s_t \mid o_{1:t})\) from POMDP can be written as task inference \(\hat p(z_t \mid s_{1:t},a_{1:t},r_{1:t})\) , where posterior sampling is used for exploration in new tasks.</li> <li> <strong>Variational approach</strong> related to the prior work on solving POMDPs is used to estimate belief over task.</li> </ul> <p><br> <br> <br></p> <hr> <h1 id="problem-statement">Problem Statement</h1> <p><br></p> <ul> <li>Assume a distribution of tasks \(p(\mathcal T)\) , where each task is a MDP.</li> <li>A task is defined as \(\mathcal T = \left\lbrace p(s_0), p(s_{t+1} \mid s_t,a_t), r(s_t,a_t) \right\rbrace\) . Note that <strong>varying transition functions</strong> and <strong>varying reward functions</strong> constitute task distribution.</li> <li>Context \(c\) is referred to the history of past transitions. Let \(c^{\mathcal T}_ n = (s_n, a_n, r_n, s_n')\) be one transition in the task \(\mathcal T\) so that \(c^{\mathcal T}_ {1:N}\) comprises the experience collected so far.</li> <li>Entirely new tasks are drawn from \(p(\mathcal T)\) at test-time.</li> </ul> <p><br> <br> <br></p> <hr> <h1 id="probabilistic-latent-context">Probabilistic Latent Context</h1> <p><br></p> <h3 id="modeling-and-learning-latent-contexts">Modeling and Learning Latent Contexts</h3> <p>An amortized variational inference approach is used to train an <em>inference network</em> \(q_\phi (z \mid c)\) , parameterized by \(\phi\) , that estimates the posterior \(p(z \mid c)\) . Let’s first check how variational inference approach works in VAE.</p> <p><br></p> <p><strong>How variational inference approach works in VAE ?</strong></p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/PEARL/VAE-480.webp 480w, /assets/img/PEARL/VAE-800.webp 800w, /assets/img/PEARL/VAE-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/PEARL/VAE.jpg" class="img-fluid" width="100%" height="auto" title="VAE" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from [Source](https://imgur.com/PhHb2aF) </div> <p>To estimate VAE parameters with maximal likelihood method, we need to maximize marginal log-likelihood \(\mathrm{log} p(x) = \mathcal{log} p(x \mid z)\ p(z)\) . As it is hard to calculate, we approximate the distribution \(p(z)\) to tractable distribution \(q(z \mid x)\) , which is called a variational inference. The evidence lower bound (ELBO) is calculated as RHS of the (\ref{eqn:ELBO}).</p> \[\begin{equation}\label{eqn:ELBO} \mathrm{log}\ p(x) \geq \mathbb E_{z \sim q(z \mid x)} \left[ \mathrm{log}\ p(x \mid z) \right] - \mathrm{ D_{KL}} (q(z \mid x) \parallel p(z)) \end{equation}\] \[\begin{align*} \text{proof)}\ \mathrm{log}\ p(x) &amp; = \int q(z \mid x) \mathrm{log}\ \frac{p(x,z)}{q(z \mid x)}\, dz - \int q(z \mid x) \mathrm{log}\ \frac{p(z \mid x)}{q(z \mid x)}\, dz \\ &amp; = \int q(z \mid x) \mathrm{log}\ \frac{p(x,z)}{p(z)}\, dz + \int q(z \mid x) \mathrm{log}\ \frac{p(z)}{q(z \mid x)}\, dz - \int q(z \mid x) \mathrm{log}\ \frac{p(z \mid x)}{q(z \mid x)}\, dz \\ &amp; = \mathbb E_{z \sim q(z \mid x)} \left[ \mathrm{log}\ p(x \mid z) \right] - \mathrm{D_{KL}} (q(z \mid x) \parallel p(z)) + \mathrm{D_{KL}} (q(z \mid x) \parallel p(z \mid x)) \end{align*}\] <p>By maximizing the ELBO which is easy to calculate, we can maximize marginal log-likelihood \(\mathrm{log} p(x)\) . As the objective function used in deep learning is loss function, we can rewrite (\ref{eqn:ELBO}) as (\ref{eqn:Loss}).</p> \[\begin{equation}\label{eqn:Loss} L = - \mathbb E_{z \sim q(z \mid x)} \left[ \mathrm{log}\ p(x \mid z) \right] + \mathrm{ D_{KL}} (q(z \mid x) \parallel p(z)) \end{equation}\] <p>Specifically, \(- \mathbb E_{z \sim q(z \mid x)} \left[ \mathrm{log}\ p(x \mid z) \right]\) is a <strong>reconstruction term</strong> that calculates the cross-entropy between encoder and decoder. \(\mathrm{D_{KL}} (q(z \mid x) \parallel p(z))\) is a <strong>regularization term</strong> that constrains \(q(z \mid x)\) to be similar to the prior \(p(z)\) (can be any tractable distribution). Meanwhile, maximizing the ELBO is equivalent to <strong>minimizing the difference between posterior \(p(z \mid x)\) and its estimation \(p(z \mid x)\)</strong>.</p> <p><br></p> <p><strong>Applying this idea to meta-RL problem</strong></p> <p>As minimizing the KL divergence between the posterior \(p(z \mid c)\) and the inference network \(q_\phi (z \mid c)\) can be achieved by maximizing the ELBO, we can define the variational lower bound <em>loss</em>:</p> \[\begin{equation}\label{eqn:inf-net} \mathbb E_{\mathcal T} \left[ \mathbb E_{z \sim q_\phi (z \mid c^{\mathcal T})} \left[ R(\mathcal T, z) + \beta\ \mathrm{D_{KL}}\left(q_\phi(z \mid c^{\mathcal T}) \parallel p(z) \right) \right]\right] \end{equation}\] <p>where \(p(z)\) is a unit Gaussian prior over \(Z\) and \(R(\mathcal T, z)\) could be a variety of objectives, such as reconstructing the MDP, modeling the state-action value functions or maximizing returns through the policy over the distribution of tasks. The KL divergence term can also be interpreted as the result of a variational approximation to the <em>information bottleneck</em> that <strong>constrains \(z\) to contain only information from the context that is necessary to adapt to the task at hand</strong>, mitigating overfitting to training tasks (by Lagrange multiplier \(\beta\) .</p> <p><br></p> <p><strong>Designing the architecture of the inference network</strong></p> <p>An encoding of a fully observed MDP should be <strong>permutation invariant</strong>: the order of a collection of transitions \(\left\lbrace s_i, a_i, s_i', r_i \right\rbrace\) doesn’t matter when used to encode MDP (task inference, value function, etc.). (Because, a single transition contains all the information that makes task distribution: transition function, reward function) With this observation, a product of independent factors consitutes permutation-invariant inference network \(q_\phi(z \mid c_{1:N})\) as</p> \[\begin{equation} q_\phi(z \mid c_{1:N}) \propto \prod_{n=1}^N \Psi_\phi(z \mid c_n). \end{equation}\] <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/PEARL/inference-network.PNG-480.webp 480w, /assets/img/PEARL/inference-network.PNG-800.webp 800w, /assets/img/PEARL/inference-network.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/PEARL/inference-network.PNG" class="img-fluid" width="100%" height="auto" title="inference-network" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables" </div> <p>To keep the method tractable, they use Gaussian factors \(\Psi_\phi(z \mid c_n) = \mathcal N (f_\phi^\mu (c_n), f_\phi^\sigma (c_n))\) , which result in a Gaussian posterior.</p> <p><br> <br></p> <h3 id="posterior-sampling-and-exploration-via-latent-contexts">Posterior Sampling and Exploration via Latent Contexts</h3> <p>Modeling the latent context as probabilistic allows us to make use of posterior sampling for efficient, <strong>temporally extended exploration</strong> at meta-test time. In this paper, PEARL directly infers a posterior over the latent context \(Z\) , which may encode the MDP itself if optimized for reconstruction, optimal behaviors if optimized for the policy, or the value function if optimized for a critic (actual implementation of PEARL as (\ref{eqn:critic-loss})). During meta-training, the prior distribution over tasks is captured with training tasks and also learns to efficiently use experience to infer new tasks. At meta-test time, \(z\) is sampled from the prior and a trajectory is collected according to \(z\) . Then <strong>using the collected experience to update the posterior</strong> and continue exploring coherently in a manner that <strong>acts more and more optimally as our belief narrows</strong>.</p> <p><br> <br> <br></p> <hr> <h1 id="off-policy-meta-reinforcement-learning">Off-Policy Meta-Reinforcement Learning</h1> <p><br></p> <p>There has been two challenges to design off-policy meta-RL algorithms:</p> <ul> <li>The distribution of data used for adaptation need to match across meta-training and meta-test. (Lead to data inefficiency during meta-training)</li> <li>Meta-RL requires the policy to reason about <em>distributions (over tasks, states and actions)</em> , so as to learn effective stochastic exploration strategies. (Using value-based RL in meta-learning is ineffective, <strong>especially in reasoning about task during meta-test</strong>)</li> </ul> <p>The main insight in designing an off-policy meta-RL method with the probabilistic context is that <strong>the data used to train the encoder need not be the same as the data used to train the policy</strong>. The policy can treat the context \(z\) as part of the state in an off-policy RL loop (as task inference is the only on-policy process that needs distribution match of data), while the stochasticity of the exploration process is provided by the uncertainty in the encoder \(q(z \mid c)\) (on-policy task inference via posterior sampling). The actor and critic are always trained with off-policy data from the <strong>entire replay buffer \(\mathcal B\)</strong> . However, the encoder is trained with context batches sampled from a sampler \(\mathcal S_c\) , where \(\mathcal S_c\) is sampled from a replay buffer of <strong>recently collected data</strong> retains on-policy performance with better efficiency. (An in-between strategy between off-policy \(\mathcal S_c\) and strict on-policy \(\mathcal S_c\))</p> <p>The training procedure is summarized in Figure 2 and Algorithm 1. Meta-testing is described in Algorithm 2.</p> <div class="row justify-content-center"> <div class="col-7"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/PEARL/PEARL-meta-training.PNG-480.webp 480w, /assets/img/PEARL/PEARL-meta-training.PNG-800.webp 800w, /assets/img/PEARL/PEARL-meta-training.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/PEARL/PEARL-meta-training.PNG" class="img-fluid" width="100%" height="auto" title="Meta-training procedure" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-5"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/PEARL/PEARL-training-algorithm.PNG-480.webp 480w, /assets/img/PEARL/PEARL-training-algorithm.PNG-800.webp 800w, /assets/img/PEARL/PEARL-training-algorithm.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/PEARL/PEARL-training-algorithm.PNG" class="img-fluid" width="100%" height="auto" title="Meta-training algorithm" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables" </div> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/PEARL/PEARL-testing-algorithm.PNG-480.webp 480w, /assets/img/PEARL/PEARL-testing-algorithm.PNG-800.webp 800w, /assets/img/PEARL/PEARL-testing-algorithm.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/PEARL/PEARL-testing-algorithm.PNG" class="img-fluid" width="100%" height="auto" title="Meta-testing algorithm" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables" </div> <p><br> <br></p> <h3 id="implementation">Implementation</h3> <ul> <li>Algorithm is built on top of the <strong><a href="https://thisiswooyeol.github.io/projects/SoftActorCritic/"><code class="language-plaintext highlighter-rouge">soft actor-critic algorithm (SAC)</code></a>.</strong> </li> <li>Parameters of the inference network \(q(z \mid c)\) , actor \(\pi_\theta(a \mid s, z)\) and critic \(Q_\theta(s,a,z)\) are optimized jointly as Figure 2 with reparameterization trick on sampling context variable \(z\) .</li> <li>Inference network is trained to <strong>encode the value function</strong> using gradients from the Bellman update for the critic (line 19 of Algorithm 1).</li> <li>The critic loss and the actor loss are written as</li> </ul> \[\begin{align}\label{eqn:critic-loss} &amp; \mathcal L_{\mathrm{critic}} = \mathbb E_{(s,a,r,s') \sim \mathcal B, z \sim q_\phi(z \mid c)} \left[ Q_\theta(s,a,z) - (r+\bar{V}(s',\bar{z}))\right]^2 \\ &amp; \mathcal L_{\mathrm{actor}} = \mathbb E_{s \sim \mathcal B, a \sim \pi_\theta, z \sim q_\phi(z \mid c)} \left[ \mathrm{D_{KL}} \left( \pi_\theta(a \mid s, \bar{z}) \parallel \frac{\mathrm{exp} (Q_\theta(s,a,\bar{z}))}{\mathcal Z_\theta(s)} \right)\right] \end{align}\] <p>where \(\bar{V}\) is a target network and \(\bar{z}\) indicates that gradients are not being computed through it (as a computational graph on Figure 2).</p> <ul> <li>The context data sampler \(\mathcal S_c\) samples uniformly from the most recently collected batch of data (line 13 of Algorithm 1).</li> <li>The batch of data \(\mathcal B^i\) is recollected every 1000 meta-training optimization steps.</li> </ul> <p><br> <br> <br></p> <hr> <h1 id="experiments">Experiments</h1> <p><br></p> <h3 id="sample-efficiency-and-performance">Sample Efficiency and Performance</h3> <p><br></p> <div class="row justify-content-center"> <div class="col-8"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/PEARL/PEARL-exp-continuous.PNG-480.webp 480w, /assets/img/PEARL/PEARL-exp-continuous.PNG-800.webp 800w, /assets/img/PEARL/PEARL-exp-continuous.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/PEARL/PEARL-exp-continuous.PNG" class="img-fluid" width="100%" height="auto" title="Meta-learning continuous control" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables" </div> <p><strong>Experimental Setup.</strong></p> <ul> <li>6 locomotion task families <strong>require adaptation across reward functions or across dynamics</strong>.</li> <li>Baseline algorithms: gradient-based(ProMP, MAML-TRPO), recurrence-based( with PPO)</li> <li>Attempted to adapt recurrent DDPG, but not work (due to the distribution mismatch in the adaptation data and the difficulty of training with trajectories rather than decorrelated transitions)</li> </ul> <p><br></p> <p><strong>Results.</strong> PEARL uses 20-100x fewer samples during meta-training than previous meta-RL approaches while improving final asymptotic performance by 50-100% in five of six domains.</p> <p><br> <br></p> <h3 id="posterior-sampling-for-exploration">Posterior Sampling For Exploration</h3> <p><br></p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/PEARL/PEARL-exp-sparse.PNG-480.webp 480w, /assets/img/PEARL/PEARL-exp-sparse.PNG-800.webp 800w, /assets/img/PEARL/PEARL-exp-sparse.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/PEARL/PEARL-exp-sparse.PNG" class="img-fluid" width="100%" height="auto" title="Sparse 2D navigation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables" </div> <ul> <li>While they aim to adapt to new tasks with spare rewards, meta-training with sparse rewards is expremely difficult. For simplicity they assume access to the dense reward (the negative distance to the goal as the reward) during meta-training, as done by <a href="https://arxiv.org/abs/1802.07245" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">MAESN</code></a>.</li> <li>MAESN : models probabilistic task variables, performs on-policy gradient-based meta-learning</li> <li>PEARL can <strong>adapt to the new sparse goal in fewer trajectories</strong> and also <strong>outperforms MAESN in terms of final performance</strong>.</li> <li>PEARL is also more efficient during meta-training.</li> </ul> <p><br> <br></p> <h3 id="ablations">Ablations</h3> <p><br></p> <div class="row justify-content-center"> <div class="col-4"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/PEARL/PEARL-exp-encoder.PNG-480.webp 480w, /assets/img/PEARL/PEARL-exp-encoder.PNG-800.webp 800w, /assets/img/PEARL/PEARL-exp-encoder.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/PEARL/PEARL-exp-encoder.PNG" class="img-fluid" width="100%" height="auto" title="Recurrent encoder ablation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-4"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/PEARL/PEARL-exp-sampler.PNG-480.webp 480w, /assets/img/PEARL/PEARL-exp-sampler.PNG-800.webp 800w, /assets/img/PEARL/PEARL-exp-sampler.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/PEARL/PEARL-exp-sampler.PNG" class="img-fluid" width="100%" height="auto" title="Context sampling ablation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-4"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/PEARL/PEARL-exp-deterministic.PNG-480.webp 480w, /assets/img/PEARL/PEARL-exp-deterministic.PNG-800.webp 800w, /assets/img/PEARL/PEARL-exp-deterministic.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/PEARL/PEARL-exp-deterministic.PNG" class="img-fluid" width="100%" height="auto" title="Deterministic latent context" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables" </div> <p><strong>Inference network architecture.</strong></p> <ul> <li>Compare permutation-invariant encoder for the latent context \(Z\) to <strong>RNN encoder</strong>. There are two options for sampling the RL batch: 1) <strong>unordered transitions</strong> as in PEARL (“RNN tran”) 2) sets of <strong>trajectories</strong> (“RNN traj”)</li> <li>“RNN tran” results in comparable performance to PEARL, at the cost of <strong>slower optimization</strong>.</li> <li>“RNN traj” results in steep drop in performance.</li> <li>The result demonstrates <strong>the importance of decorrelating the samples</strong> used for the RL objective.</li> </ul> <p><br></p> <p><strong>Data sampling strategies.</strong></p> <ul> <li>Ablate the <strong>context sampling strategy</strong> used during training. Three options for \(\mathcal S_c\) are: 1) original : sample recently colleted data, distinct from the RL batch 2) “off-policy” : sample fully off-policy data, distinct from the RL batch 3) “off-policy RL-batch” : sample fully off-policy data, identical to the RL batch</li> <li>The results demonstrates <strong>the importance of careful data sampling in off-policy meta-RL</strong>.</li> </ul> <p><br></p> <p><strong>Deterministic context.</strong></p> <ul> <li>“deterministic” : let the distribution \(q_\phi (z \mid c)\) to a point estimate.</li> <li>With no stochasticity in the latent context variable, <strong>the only stochasticity comes from the policy and is thus time-invariant, hindering temporally extended exploration</strong>.</li> </ul> <p><br> <br> <br></p> <hr> <h1 id="discussion">Discussion</h1> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Wooyeol Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script async src="https://rum.cronitor.io/script.js"></script> <script>window.cronitor=window.cronitor||function(){(window.cronitor.q=window.cronitor.q||[]).push(arguments)},cronitor("config",{clientKey:""});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>