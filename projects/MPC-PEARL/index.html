<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="HTJxFQYrIX3p9c68oCi3cp22YWIGnLSAAD3wrcxR4zQ"> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>MPC-PEARL | Wooyeol Lee</title> <meta name="author" content="Wooyeol Lee"> <meta name="description" content="Review on " infusing model predictive control into meta-reinforcement learning for mobile robots in dynamic environments> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, RL, Reinforcement, Learning, AI, Robot, work-out, coffee, travel"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9C%88%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://thisiswooyeol.github.io/projects/MPC-PEARL/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Wooyeol Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">MPC-PEARL</h1> <p class="post-meta">November 21, 2022</p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2022   ·   <i class="fa-solid fa-tag fa-sm"></i> papers review   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="tldr">TL;DR:</h2> <ul> <li>MPC-PEARL: a novel combination of meta-RL and MPC with <strong>event-triggered probabilistic switching between the two modules</strong>.</li> <li> <strong>Event-triggered switching</strong> makes up for ineffective behaviors of PEARL with MPC, <strong>encouraging exploration of potentially high-reward regions</strong>.</li> <li> <strong>Randomization layer</strong> induces MPC-PEARL to learn frequently from actions generated by MPC, thereby <strong>complementing suboptimal MPC actions caused by the limited prediction horizon</strong>.</li> <li> <strong>GPR</strong> and <strong>CVaR constraint</strong> are used in MPC <strong>to build safe region constraint with unknown motion of dynamic obstacles</strong>.</li> <li>An online adaptdation scheme enables the robot <strong>to infer and adapt to a new task within a single trajectory</strong>.</li> </ul> <p><br> <br> <br></p> <hr> <h2 id="introduction">Introduction</h2> <p><br></p> <h3 id="limitations-of-meta-rl--mpc">Limitations of meta-RL &amp; MPC</h3> <p>As it is crucial for mobile robots to adapt quickly to environmental changes, meta-RL algorithms (e.g. PEARL) are suitable for robot navigation problems. However, the <strong>meta-learned policy may be conservative</strong> in some situations. Another sequential decision-making tool, which is MPC, can be used to infer unknown parts of the system or environmental model with various ML techniques. But, learning-based MPC techniques are <strong>computationally demanding</strong> and MPC optimizes an action sequence within a <strong>fixed short horizon, causing suboptimal, myopic behaviors in general</strong>.</p> <p><br> <br></p> <h3 id="motivation-of-mpc-pearl">Motivation of MPC-PEARL</h3> <p>There have been a few attemps to use MPC in model-based meta-RL or meta-learning methods, such as using learned MPC in real-time decision making. However, none of them utilizes the actions generated by MPC in the <strong>training stage</strong>. Motivated by this observation, the authors proposed a systematic approach to <strong>infusing MPC into the meta-RL training process</strong> for mobile robots in environments cluttered with moving obstacles.</p> <p><br> <br> <br></p> <hr> <h2 id="preliminaries">Preliminaries</h2> <p><br></p> <h3 id="motion-control-in-dynamic-environments">Motion Control in Dynamic Environments</h3> <p><br></p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/MPC-PEARL/MPC-PEARL-dynamic-system.PNG-480.webp 480w, /assets/img/MPC-PEARL/MPC-PEARL-dynamic-system.PNG-800.webp 800w, /assets/img/MPC-PEARL/MPC-PEARL-dynamic-system.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/MPC-PEARL/MPC-PEARL-dynamic-system.PNG" class="img-fluid" width="100%" height="auto" title="configuration of the motion control problem" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments" </div> <p><strong>The motions of the robot and obstacles</strong></p> <ul> <li> <p>The motion of the robot is modeled by the following discrete-time nonlinear system:</p> <p>\(\begin{equation}\label{eqn:robot-motion} x_{t+1} = f(x_t, u_t), \end{equation}\) where \(x_t \in \mathcal X \subseteq \mathbb R^{n_x}\) and \(u_t \in \mathcal U \subseteq \mathbb R^{n_u}\) are the robot’s state and control input at stage t.</p> </li> <li>state of dynamic obstacles: \(x_{t,i}^d \in \mathbb R^{n_d}, i=1, ..., N_d\)</li> <li>state of static obstacles: \(x_i^s \in \mathbb R^{n_s}, i=1,...,N_s\)</li> <li>As a common practice, the obstacles’ motion can be tracked with high accuracy (fully observability assumption). <br> </li> </ul> <p><strong>Definition of the motion control problem</strong></p> <ul> <li>The motion control problem: MDP tuple \(\mathcal T := \left( \mathcal{S,U} , P, c, \gamma \right)\) , where \(\mathcal S \subseteq \mathbb R^{n_x+n_dN_d+n_sN_s}\) and \(\mathcal U\) are the state and action spaces, respectively, \(c\) iis a stage-wise cost function of interest.</li> <li>The MDP state: \(s_t := \left( x_t,x_{t,1}^d, ..., x_{t,N_d}^d, x_1^s,..., x_{N_s}^s \right)\) , concatenating the robot state and the states of the static and dynamic obstacles.</li> <li> <p>The stage-cost function \(c\) is chosen as follows:</p> \[\begin{equation}\label{eqn:cost} c(s_t,u_t) := l(x_t,u_t) + w_d \mathbf 1_{ \lbrace x_t \notin \mathcal X_t^d \rbrace } + w_s \mathbf 1_{ \lbrace x_t \notin \mathcal X^s \rbrace } - w_g \mathbf 1_{ \lbrace x_t \in \mathcal X^g \rbrace } , \end{equation}\] <p>where the loss function \(l\) measures the control performance, \(\mathcal X_t^d\) and \(\mathcal X^s\) denote the safe regions with respect to dynamic and static obstacles to be defined.</p> </li> <li>\(\mathcal X_t^d := \lbrace x \in \mathbb R^{n_x} \mid h_d(x,x_{t,i}^d) \geq \epsilon_i^d, i=1,...N_d \rbrace\) : The safety function is \(h_d(x,x_i^d) := \lVert p^r -p_i^d \rVert_2\) , where \(p^r\) and \(p_i^d\) represent the center of gravities of the robot and dynamic obstacle \(i\) , respectively. The threshold is \(\epsilon_i^d = r^r+r_i^d+r^\text{safe}\) , where \(r^r\) and \(r_i^d\) are the radii of balls covering the robot and the obstacle respectively. \(\mathcal X^s\) can be designed in a similar manner.</li> <li>\(\mathcal X^g := \lbrace x \in \mathbb R^{n_x} \mid \lVert p^r -p_\text{goal} \rVert_2 \leq \epsilon^g \rbrace\) : incentivizes the robot to reach a neighborhood of the goal point.</li> </ul> <p><br> <br></p> <h3 id="off-policy-meta-reinforcement-learning">Off-Policy Meta-Reinforcement Learning</h3> <p><br></p> <p>As the motion pattern of dynamic obstacles and the configuration of static obstacles (transition function \(P\) and cost function \(c\) ) vary, it is desired to train the robot via meta-RL. To enhance sample efficiency in meta-RL, adopt <a href="https://thisiswooyeol.github.io/projects/PEARL/"><code class="language-plaintext highlighter-rouge">PEARL</code></a>, which is a state-of-the-art off-policy meta-RL algorithm. (You can see detailed explanations on <code class="language-plaintext highlighter-rouge">PEARL</code> algorithm at the link above.)</p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/MPC-PEARL/PEARL-on-navigation-problem.PNG-480.webp 480w, /assets/img/MPC-PEARL/PEARL-on-navigation-problem.PNG-800.webp 800w, /assets/img/MPC-PEARL/PEARL-on-navigation-problem.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/MPC-PEARL/PEARL-on-navigation-problem.PNG" class="img-fluid" width="100%" height="auto" title="PEARL on robot navigation task" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments" </div> <p>Fig. 2 shows how the PEARL policy operates in the robot navigation task. This example indicates that <strong>PEARL policies may induce overly conservative behaviors</strong> to bypass regions cluttered with moving obstacles.</p> <p><br> <br> <br></p> <hr> <h2 id="infusing-mpc-into-meta-rl">Infusing MPC into Meta-RL</h2> <p><br></p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/MPC-PEARL/Overview-of-MPC-PEARL.PNG-480.webp 480w, /assets/img/MPC-PEARL/Overview-of-MPC-PEARL.PNG-800.webp 800w, /assets/img/MPC-PEARL/Overview-of-MPC-PEARL.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/MPC-PEARL/Overview-of-MPC-PEARL.PNG" class="img-fluid" width="100%" height="auto" title="Overview of MPC-PEARL" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments" </div> <p>To resolve the limited nagivation performance of PEARL, learning-based MPC is combined to <strong>provide it with transition samples in case a predefined event occurs</strong>. (The authors considered the following two events that are ineffectively handled by PEARL: (Event 1) collision with a dynamic obstacle, and (Event 2) reaching the neighboring region of the goal point.) If the MPC module is activated, a carefully designed MPC problem is solved using <strong>GPR result of inferring the motion of obstacles</strong>. Otherwise, the original PEARL algorithm chooses an action. Note that <strong>the randomization layer induces MPC-PEARL to learn frequently from actions generated by MPC</strong>.</p> <p><br> <br></p> <h3 id="mpc-pearl-algorithm">MPC-PEARL Algorithm</h3> <p><br></p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/MPC-PEARL/MPC-PEARL-meta-training.PNG-480.webp 480w, /assets/img/MPC-PEARL/MPC-PEARL-meta-training.PNG-800.webp 800w, /assets/img/MPC-PEARL/MPC-PEARL-meta-training.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/MPC-PEARL/MPC-PEARL-meta-training.PNG" class="img-fluid" width="100%" height="auto" title="MPC-PEARL meta-training" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments" </div> <ul> <li>(line 4-17) Transition samples are collected for each task \(\mathcal T^i\) to build a task-specific replay-buffer \(\mathcal H^i\) .</li> <li>(line 9-11) A carefully designed MPC predicts the motion of dynamic obstacles via GPR and to avoid collisions via CVaR constraints.</li> <li>(line 20) \(\mathcal S_c\) is chosen to generate transition samples uniformly from the most recent data collection stage.</li> <li>(line 22-23) The policy is trained using <a href="https://thisiswooyeol.github.io/projects/SoftActorCritic/"><code class="language-plaintext highlighter-rouge">soft actor-critic (SAC)</code></a> and the context encoder network is trained as in <a href="https://thisiswooyeol.github.io/projects/PEARL/"><code class="language-plaintext highlighter-rouge">PEARL</code></a>.</li> </ul> <p><br></p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/MPC-PEARL/MPC-PEARL-meta-testing.PNG-480.webp 480w, /assets/img/MPC-PEARL/MPC-PEARL-meta-testing.PNG-800.webp 800w, /assets/img/MPC-PEARL/MPC-PEARL-meta-testing.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/MPC-PEARL/MPC-PEARL-meta-testing.PNG" class="img-fluid" width="100%" height="auto" title="MPC-PEARL meta-testing" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments" </div> <ul> <li>By <strong>omitting the MPC module during meta-testing</strong>, a significant amount of <strong>computation time can be saved</strong>.</li> <li>(line 5-6) The online adaptation scheme, performed <strong>within a single trajectory</strong>, is crucial for some practical applications where <strong>each episode is less likely to be repeated</strong> due to its consistently evolving environments.</li> </ul> <p><br> <br></p> <h3 id="learning-based-mpc-with-cvar-constraints">Learning-Based MPC with CVaR Constraints</h3> <p><br></p> <p>As PEARL does not use the known system dynamcis that is often available for various practical robots, authors adpoted a learning-based MPC technique that solves the given task and uses model information in a receding horizon fashion:</p> \[\begin{align}\label{eqn:MPC-det} \begin{split} \underset{\mathbf u}{\mathrm{min}}\ &amp; J(x_t, \mathbf u) = l_f(x_{t+K \mid t} ) + \sum_{k=0}^{K-1} l(x_{t+k \mid t} , u_{t+k \mid t} ) \\ \text{s.t.}\ &amp; x_{t+k+1 \mid t} = f(x_{t+K \mid t} , u_{t+k \mid t}) \\ &amp; x_{t \mid t} = x_t \\ &amp; x_{t+k \mid t} \in \mathcal X_{t+k}^d \cap \mathcal X^s \\ &amp; u_{u+k \mid t} \in \mathcal U, \end{split} \end{align}\] <p>where \(\mathbf u = (u_{t \mid t} , ... , u_{t+K-1 \mid t} )\) is a control sequence over the \(K\) prediction horizon, (3b) and (3e) must be satisfied for \(k=0,..., K-1\) and (3d) (safe region constraint) must hold for \(k=0,...K\) . Unfortunately, the MPC problem in the above form is impossible to solve because in general <strong>the safe region \(\mathcal X_{t+k}^d\) is unknown</strong>; it includes information about the future motion of dynamic obstacles. Here are how the authors solved this problem.</p> <p><br></p> <p><strong>Learning the motion of obstacles via GPR <a href="http://mlg.eng.cam.ac.uk/pub/pdf/Ras04.pdf" rel="external nofollow noopener" target="_blank">(<code class="language-plaintext highlighter-rouge">What is GPR?</code>)</a></strong></p> <ul> <li>GPR is performed online using a training dataset \(\mathcal D_t = \lbrace x_{t-l}^d, v_{t-l}^d \rbrace_{l=1}^{N_\mathrm{GP}} = \lbrace \tilde{\mathrm{x}}, \tilde{\mathrm{v}} \rbrace\) (\(N_\mathrm{GP}\) most recent observations of the obstacles’ state transitions), where \(v_{t-l}^d := x_{t-l+1}^d - x_{t-l}^d\) .</li> <li> <p>The GPR problem is defined for each \(j\) th entry of \(\mathbf v\) as</p> \[\begin{align*} \ \ \tilde{\mathrm{v}}_ j = &amp; \mathbf{v}_ j (\tilde{\mathrm{x}}_ j) + \omega_j.\\ \text{where} \ \ &amp; \mathbf{v}_ j (\mathrm{x}) \sim \mathcal{GP}(\mathbf{0}, k_j(\mathrm{x, x'})) \\ &amp; k_j(\mathrm{x,x'}) = \sigma_f^2 \ \mathrm{exp} \left( - \frac{ \lVert \mathrm{x}_ j - \mathrm{x_j'} \rVert }{2\lambda^2} \right) \\ &amp; \omega_j \sim \mathcal N \left( \mathbf{0}, \text{diag} \left[ \sigma_{\omega, j}^2, \ldots , \sigma_{\omega, j}^2 \right] \right) \end{align*}\] </li> <li> <p>The corresponding approximation of \(\mathbf v\) at an arbitrary test point \(\mathbf x\) is then represented by \(\mathbf v(\mathbf x) \sim \mathcal N (\mu^v(\mathbf x), \Sigma^v(\mathbf x))\) , where \(\mu^v\) and \(\Sigma^v\) are computed as</p> \[\begin{align*} \mu_j^v(\mathbf x) &amp;= K_j(\mathbf x, \tilde{\mathrm{x}}) \left[ K_j(\tilde{\mathrm{x}}, \tilde{\mathrm{x}}) + \sigma_{\omega, j}^2 I \right]^{-1} \tilde{\mathrm{v}}_ j \\ \Sigma_j^v(\mathbf x) &amp;= k_j(\mathbf{x, x}) - K_j(\mathbf x, \tilde{\mathrm{x}}) \left[ K_j(\tilde{\mathrm{x}}, \tilde{\mathrm{x}}) + \sigma_{\omega, j}^2 I \right]^{-1} K_j(\tilde{\mathrm{x}}, \mathbf x) \end{align*}\] </li> <li> <p>Starting from \(\hat{x}_ t^d \sim \mathcal N(x_t^d, \mathbf 0)\) , the approximated state of the obstacle \(\hat{x}_ {t+k+1}^d \sim \mathcal N(\mu_{t+k+1}, \Sigma_{t+k+1})\) is obtained by propagating the state vector using the estimated velocity evaluated at the current mean state:</p> \[\mu_{t+k+1} = \mu_{t+k} + \mu^v(\mu_{t+k}), \ \ \Sigma_{t+k+1} = \Sigma_{t+k} + \Sigma^v(\mu_{t+k})\] </li> </ul> <p><br></p> <p><strong>CVaR constraints for safety</strong></p> <ul> <li>Due to the <strong>stochastic nature of \(\hat{\mathcal X}_ {t+k}^d\)</strong> , imposing the deterministic constraint \(x_{t+k \mid t} \in \hat{\mathcal X}_ {t+k}^d\) is likely to cause infeasibility or an overly conservative solution.</li> <li>As a remedy a <strong>probabilistic constraint</strong> is used: <strong>CVaR constraint</strong> </li> <li>CVaR of a random loss \(X\) is its expected value within the \((1-\alpha)\) worst-case quantile and is defined as \(\mathrm{CVaR}_ \alpha [X]:= \mathrm{min}_ {\xi \in \mathbb R} \mathbb E [\xi + (X - \xi)^+ / (1 -\alpha)]\) , where \(\alpha \in (0,1)\) and \((x)^+ = \mathrm{max} \lbrace x,0 \rbrace\) (Check <a href="https://www-iam.mathematik.hu-berlin.de/~romisch/SP01/Uryasev.pdf" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">Conditional Value-at-Risk (CVAR): Algorithms and Applications</code></a> by Stan Uryasev if you want more detailed explanations.</li> <li>CVaR is a <strong>convex risk measure</strong> and <strong>discerns a rare but catastrophic event</strong> in the worst-case tail distribution unlike chance constraits. (As CVaR is a expectation value)</li> <li> <p>By replacing a random loss \(X\) in CVaR definition with \(L(x_t, \hat{x}_ t^d) := \mathrm{max}_ {i=1,...N_d} \lbrace \epsilon_i^d - h_d(x_t,\hat{x}_ {t,i}^d) \rbrace\) , the deterministic safe region constrint (3d) now can be written in probabilistic as</p> \[\mathrm{CVaR}_ \alpha \left[ L(x_{t+k \mid t}, \hat{x}_ {t+k}^d) \right] = \underset{\xi \in \mathbb R}{\mathrm{min}} \left( \xi + \frac{1}{1-\alpha} \mathbb E \left[ \left( L(x_{t+k \mid t}, \hat{x}_ {t+k}^d) - \xi \right)^+ \right] \right) \leq \delta_{\text{CVaR}},\] <p>where \(\delta_\text{CVaR}\) is a user-specific non-negative threshold representing <strong>the maximum tolerable risk level</strong>.</p> </li> <li> <p>CVaR constraint can be approximated by <strong>sample average approximation (SAA)</strong> using a GPR distribution samples \(\lbrace \hat{x}_ {t+k}^{d, (m)} \rbrace_{m=1}^{M_\mathrm{SAA}}\) . The resulting <strong>GP-MPC</strong> formulation is</p> \[\begin{align} \begin{split} \underset{\mathbf u, \xi}{\mathrm{min}}\ &amp; J(x_t, \mathbf u) \\ \text{s.t.}\ &amp; \xi_k + \sum_{m=1}^{M_\text{SAA}} \frac{ \left(L(x_{t+k \mid t}, \hat{x}_ {t+k}^{d,(m)}) - \xi_k \right)^+}{M_{\text{SAA}}(1-\alpha)} \leq \delta_\text{CVaR} \\ &amp; x_{t+k \mid t} \in \mathcal X^s \\ &amp; (3b), (3c), \text{and} (3e). \end{split} \end{align}\] </li> </ul> <p><br> <br> <br></p> <hr> <h2 id="simulation-results">Simulation Results</h2> <p><br></p> <h3 id="simulation-details">Simulation Details</h3> <p><br></p> <p><strong>Robot Dynamics</strong></p> <ul> <li>state: the robot’s CoG \((x_t^c, y_t^c)\) and the heading angle \(\theta_t\)</li> <li>control input (available action): the velocity \(v_t\) and the steering angle of the front wheel \(\delta_t\) , where the action space is set to \(\mathcal U:= [9,v_\text{max}] \times [-\frac{\pi}{2}, \frac{\pi}{2}]\) .</li> <li>The transition model of robot in \eqref{eqn:robot-motion} are now given by</li> </ul> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/MPC-PEARL/Kinematics-of-lateral-vehicle-motion.PNG-480.webp 480w, /assets/img/MPC-PEARL/Kinematics-of-lateral-vehicle-motion.PNG-800.webp 800w, /assets/img/MPC-PEARL/Kinematics-of-lateral-vehicle-motion.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/MPC-PEARL/Kinematics-of-lateral-vehicle-motion.PNG" class="img-fluid" width="100%" height="auto" title="Kinematics of lateral vehicle motion" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from R. Rajamani, "Vehicle Dynamics and Control" </div> \[\begin{align*} x_{t+1}^c &amp; = x_t^c + T_s v_t \mathrm{cos}(\theta_t + \beta_t) \\ y_{t+1}^c &amp; = y_t^c + T_s v_t \mathrm{sin}(\theta_t + \beta_t) \\ \theta_{t+1}^c &amp; = \theta_t + T_s \frac{v_t \mathrm{cos}(\beta_t)}{L} \mathrm{tan}(\delta_t), \end{align*}\] <p>where \(T_s\) is the sample time and the slip angle \(\beta_t\) is defined as \(\beta_t := \mathrm{tan}^{-1}(\frac{\mathrm{tan} \delta_t}{2})\) .</p> <p><br></p> <p><strong>MPC-PEARL Details</strong></p> <ul> <li>The MPC module is activated with a probability of \(\epsilon \in \lbrace 0.2, 0.5, 0.8, 1 \rbrace\) when the event occur.</li> <li>The loss function in \eqref{eqn:cost} is chosen to speed up navigation with limited control energy as \(l(x_t, u_t) := \lVert x_t - x_\text{goal} \rVert_Q^2 + \lVert u_t \rVert_R^2\) where \(Q =\mathrm{diag}[1,1,0], R=0.2I_{n_u}\) .</li> <li>The terminal cost function in \eqref{eqn:MPC-det} was chosen similarly to the loss function in the cost function as \(l_f(x) := \lVert x-x_\text{goal} \rVert_{Q_f}^2\) with \(Q_f = 20 Q\) .</li> <li>The GPR is performed using the latest \(N_\text{GP} = 10\) observations for predicting the motion of the closest \(N_\text{adapt} = 2\) obstacles for a planning horizon of \(K = 10\) .</li> </ul> <p><br></p> <p><strong>Baseline Algorithms and Performance Metrics</strong></p> <p>MPC-PEARL is compared to PEARL, GP-MPC, MPC-SAC and GRBAL (Model-based meta-RL method with gradient-based MAML), using the following metrics:</p> <ul> <li> <strong><em>Success Rate (SR)</em></strong>: reaching the goal point &amp; no collision occurs</li> <li> <strong><em>Travel Time (TT)</em></strong>: time to reach the goal without any collision</li> <li> <strong><em>Collision-Free Rate (CFR)</em></strong>: # no collision tasks / # all tasks</li> <li> <strong><em>Success Weighted by Path Length (SPL)</em></strong>: \(\frac{1}{N} \sum_{i=1}^N S_i \frac{l_i}{\mathrm{max}}(p_i, l_i)\) where \(l_i\) is the shortest-path distance from the agent’s starting position to the goal in episode i, \(p_i\) is the length of path actually taken by agent and \(S_i\) is a binary indicator of success in episode i.</li> <li> <strong><em>Mean Goal Distance (MGD)</em></strong>: the average euclidean distance between the robot and the goal</li> </ul> <p><br> <br></p> <h3 id="restaurant-environment">Restaurant Environment</h3> <p><br></p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/MPC-PEARL/Experiment-restaurant-graph.PNG-480.webp 480w, /assets/img/MPC-PEARL/Experiment-restaurant-graph.PNG-800.webp 800w, /assets/img/MPC-PEARL/Experiment-restaurant-graph.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/MPC-PEARL/Experiment-restaurant-graph.PNG" class="img-fluid" width="100%" height="auto" title="Restaurant Environment Results" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/MPC-PEARL/Experiment-restaurant-table.PNG-480.webp 480w, /assets/img/MPC-PEARL/Experiment-restaurant-table.PNG-800.webp 800w, /assets/img/MPC-PEARL/Experiment-restaurant-table.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/MPC-PEARL/Experiment-restaurant-table.PNG" class="img-fluid" width="100%" height="auto" title="Restaurnat Environment Metrics" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments" </div> <ul> <li>Fig. 4 displays that MPC-PEARL with \(\epsilon = 0.2, 0.5\) shows the best performance not the case with \(\epsilon = 0.8\) or \(1\) , which has fewer opportunities to discover control actions that are better than MPC actions. This indicates that <strong>MPC-PEARL effectively exploits both MPC and PEARL</strong> to attain better navigation performances.</li> <li>GrBAL presents the worst performance in terms of all metrics. As GrBAL is a <strong>pure model-based method</strong>, it suffers from the bias accumulated during trajectory rollouts. Also, the lack of a systematic exploration scheme degrades the overall performance.</li> <li> <strong>The systematic infusion of MPC actions accelerates meta-training</strong>, particularly in the early stages. However, <strong>the infusion rate should be kept under a certain threshold</strong> to improve the overall performance.</li> <li> <strong>The online adaptation scheme does not significantly degrade the performance of the MPC-PEARL policy with \(\epsilon = 0.2\)</strong> although the online version uses much less information.</li> </ul> <p><br></p> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/MPC-PEARL/Experiment-restaurant-trajectories.PNG-480.webp 480w, /assets/img/MPC-PEARL/Experiment-restaurant-trajectories.PNG-800.webp 800w, /assets/img/MPC-PEARL/Experiment-restaurant-trajectories.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/MPC-PEARL/Experiment-restaurant-trajectories.PNG" class="img-fluid" width="100%" height="auto" title="Restaurant Environment Trajectories" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments" </div> <ul> <li>In Fig. 5 (a), the robot controlled by MPC is <strong>stuck in the middle due to its short horizon</strong>.</li> <li>In Fig. 5 (b), PEARL steers the robot toward a <strong>conservative path, detouring the cluttered region</strong>.</li> <li>In the case of MPC-PEARL with \(\epsilon = 0.2, 0.5, 0.8\) , <strong>the robot takes a riskier yet shorter path without a collision</strong>.</li> </ul> <p><br> <br></p> <h3 id="sidewalk-environment">Sidewalk Environment</h3> <div class="row justify-content-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/MPC-PEARL/Experiment-sidewalk-trajectories.PNG-480.webp 480w, /assets/img/MPC-PEARL/Experiment-sidewalk-trajectories.PNG-800.webp 800w, /assets/img/MPC-PEARL/Experiment-sidewalk-trajectories.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/MPC-PEARL/Experiment-sidewalk-trajectories.PNG" class="img-fluid" width="100%" height="auto" title="Sidewalk Environment Trajectories" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments" </div> <ul> <li>The <strong>agressive goal-oriented nature of GP-MPC</strong> causes a collision (SR:0.48, TT:59.62).</li> <li>PEARL displays a <strong>conservative behavior</strong> that does not lead to any collisions (CFR:0.84), but falling behind GP-MPC in terms of navigation performance (SR:0.03, SPL: 0.03, TT:249.61).</li> <li>MPC-PEARL with \(\epsilon = 0.2\) showed the best performance, balancing safety and efficiency (SR:0.67, TT:211.35, SPL:0.44).</li> </ul> </div> </article><div id="giscus_thread" style="max-width: 1200px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"thisisWooyeol/thisiswooyeol.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0" style="text-align:center"> © Copyright 2024 Wooyeol Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-S9X10ME12Y"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-S9X10ME12Y");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>