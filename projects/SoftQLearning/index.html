<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="HTJxFQYrIX3p9c68oCi3cp22YWIGnLSAAD3wrcxR4zQ"> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Soft Q-Learning | Wooyeol Lee</title> <meta name="author" content="Wooyeol Lee"> <meta name="description" content="Review on " reinforcement learning with deep energy-based policies> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, RL, Reinforcement, Learning, AI, Robot, work-out, coffee, travel"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9C%88%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://thisiswooyeol.github.io/projects/SoftQLearning/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Wooyeol Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Soft Q-Learning</h1> <p class="post-description">Review on "Reinforcement Learning with Deep Energy-Based Policies"</p> </header> <article> <h1 id="tldr">TL;DR:</h1> <ul> <li>A method for <strong>learning expressive energy-based policies for continuous states and actions</strong> </li> <li>Apply the method to learning <strong>maximum entropy policies</strong> </li> <li>Used <strong>amortized Sten variational gradient descent</strong> to obtain complex multimodal policies</li> <li> <strong>Improved exploration</strong> in the case of multimodal objectives</li> <li> <strong>compositionality</strong> via pretraining general-purpose stochastic policies</li> <li>There is a connection to <strong>actor-critic methods</strong> </li> </ul> <p><br> <br> <br></p> <hr> <h1 id="preliminaries">Preliminaries</h1> <p><br></p> <h3 id="maxmimum-entropy-reinforcement-learning">Maxmimum Entropy Reinforcement Learning</h3> <p>Maximum Entropy RL objective is</p> \[\begin{equation} \pi_\mathrm{MaxEnt}^* =\underset{\pi}{\mathrm{argmax}}\sum_t \mathbb E_{(s_t,a_t) \sim \rho_\pi} \left[\sum_{l=t}^\infty \gamma^{l-t} \mathbb E_{(s_l,a_l)} \left[r(s_t,a_t)+\alpha \mathcal H(\pi(\cdot|s_t)) \right]\right] \end{equation}\] <p>where \(\alpha\) is a temperature parameter that determines the relative importance of entropy and reward. This objective aims to maximize the entropy of the <strong>entire trajectory distribution for the policy \(\pi\)</strong>. See more details of objective with discount factor in Appendix A.</p> <p><br> <br></p> <h3 id="soft-value-functions-and-energy-based-models">Soft Value Functions and Energy-Based Models</h3> <p>General energy-based policies of the form</p> \[\begin{equation} \pi(a_t|s_t) \propto \mathrm{exp}(-\mathcal E(s_t,a_t)), \end{equation}\] <p>where \(\mathcal E\) is an energy function can represent very general class of distributions. There is a close connection between such energy-based models and <em>soft</em> versions of value functions and Q-functions, where we set \(\mathcal E(s_t,a_t) = \frac{1}{\alpha}Q_{soft}(s_t,a_t)\) and use the following theorem:</p> <p><br></p> <blockquote> <p><strong>Theorem 1.</strong> <em>Let the soft Q-function be defined by</em></p> \[\begin{equation}\label{eqn:soft-q} Q_{soft}^* (s_t,a_t)=r_t+\mathbb E_{(s_{t+1},...) \sim \rho_\pi} \left[\sum_{l=1}^\infty \gamma^l(r_{t+l}+\alpha \mathcal H(\pi_\mathrm{MaxEnt}^* (\cdot|s_{t+l})))\right], \end{equation}\] <p><em>and soft value function by</em></p> \[\begin{equation}\label{eqn:soft-value} V_{soft}^* (s_t)=\alpha \ \mathrm{log}\int_{\mathcal{A}} \mathrm{exp} \left( \frac{1}{\alpha} Q_{soft}^* (s_t,a') \right)\, da'. \end{equation}\] <p><em>Then the optimal policy for (2) is given by</em></p> \[\begin{equation}\label{eqn:opt-policy} \pi_\mathrm{MaxEnt}^* (a_t|s_t) = \mathrm{exp} \left(\frac{1}{\alpha} \left( Q_{soft}^* (s_t,a_t) - V_{soft}^* (s_t) \right) \right). \end{equation}\] </blockquote> <p>How to prove thm 1.:</p> <ul> <li>If one rewrites the maximum entropy objective with soft Q-function, the discounted maximum entropy policy objective can be defined as</li> </ul> \[\begin{equation*} J(\pi) \triangleq \sum_t \mathbb E_{(s_t,a_t) \sim \rho_\pi} \left[Q_{soft}^\pi(s_t,a_t)+\alpha \mathcal H(\pi(\cdot|s_t))\right]. \end{equation*}\] <ul> <li>Check Appendix A.1 Theorem 4. Given a policy \(\pi\) , defining a new policy \(\tilde{\pi}\) as \(\tilde{\pi} \propto \mathrm{exp} (Q_{soft}^\pi (s, \cdot), \ \forall s\) maximize the objective \(\alpha \mathcal H(\pi(\cdot \mid s)+\mathbb E_{a \sim \pi} \left[ Q_{soft}^\pi (s,a)\right]\).</li> <li>From theorem 4., by applying policy iteration \(\pi_{i+1}(\cdot \mid s) \propto \mathrm{exp}(Q_{soft}^{\pi_i}(s,\cdot))\) from an arbitrary policy \(\pi_0\) we can get \(\pi_\infty(a \mid s) \propto_a \mathrm{exp}(Q^{\pi_\infty}(s,a))\).</li> </ul> <p><br></p> <blockquote> <p><strong>Theorem 2.</strong> <em>The soft Q-function in (\ref{eqn:soft-q}) satisfies the soft bellman equation</em></p> \[\begin{equation} Q_{soft}^* (s_t,a_t)=r_t+\gamma \mathbb E_{s_{t+1} \sim p_s} \left[ V_{soft}^* (s_{t+1}) \right], \end{equation}\] <p><em>where the soft value function</em> \(V_{soft}^*\) <em>is given by (\ref{eqn:soft-val}).</em></p> </blockquote> <p>How to prove thm 2.: check Appendix A.2.</p> <p><br> <br> <br></p> <hr> <h1 id="training-expressive-energy-based-models-via-soft-q-learning">Training Expressive Energy-Based Models via Soft Q-Learning</h1> <p><br></p> <h3 id="soft-q-iteration">Soft Q-Iteration</h3> <blockquote> <p><strong>Theorem 3.</strong> <em>Soft Q-iteration. Let \(Q_{soft}(\cdot \mid \cdot)\) and \(V_{soft}(\cdot)\) be bounded and assume that \(\int_\mathcal{A} \mathrm{exp}\left(\frac{1}{\alpha}Q_{soft}(\cdot,a')\right)\, da' &lt; \infty\) and that</em> \(Q_{soft}^* &lt; \infty\) <em>exists. Then the fixed-point iteration</em></p> \[\begin{align} Q_{soft}(s_t,a_t) \gets r_t+\gamma \mathbb E_{s_{t+1} \sim p_s} \left[ V_{soft}(s_{t+1})\right], \quad \forall s_t, a_t \\ V_{soft}(s_t) \gets \alpha \ \mathrm{log} \int_\mathcal{A} \mathrm{exp}\left(\frac{1}{\alpha} Q_{soft}(s_t, a')\right)da', \quad \forall s_t \end{align}\] <p><em>converges to</em> \(Q_{soft}^*\) <em>and</em> \(V_{soft}^*\) <em>, respectively.</em></p> </blockquote> <p>How to prove thm 3.: check Appendix A.2 that soft Bellman backup operator \(\mathcal T\) is a contraction.</p> <p><br></p> <p><strong>PROBLEM</strong></p> <ol> <li>Soft Bellman backup cannot be performed exactly in <strong>continuous or large state and action spaces</strong>.</li> <li> <strong>Sampling from the energy-based model in (\ref{eqn:opt-policy}) is intractable</strong> in general.</li> </ol> <p><br> <br></p> <h3 id="soft-q-learning">Soft Q-Learning</h3> <p>To handle problem 1, this paper express the Bellman backup process as a <strong>stochastic optimization</strong>. For soft value function, expectation via importance sampling is used(Algorithm 1 line 16-17, averaged over Kv samples).</p> \[\begin{equation}\label{eqn:prac-value} V_{soft}^\theta (s_t)=\alpha \ \mathrm{log} \mathbb E_{q_{a'}} \left[\frac{\mathrm{exp}(\frac{1}{\alpha} Q_{soft}^\theta (s_t,a'))}{q_{a'}(a')} \right]. \end{equation}\] <p>While \(q_{a'}\) can be an arbitrary distribution over the action space, the current policy is used in this paper. More details are in Appendix C.2.</p> <p>For soft q-iteration, minimizing the following objective is used(Algorithm 1 line 18-19).:</p> \[\begin{equation} J_Q(\theta)=\mathbb E_{s_t \sim q_{s_t}, a_t \sim q_{a_t}} \left[ \frac{1}{2} \left(\hat Q_{soft}^{\bar{\theta}} (s_t,a_t)-Q_{soft}^\theta(s_t,a_t)\right)^2\right], \end{equation}\] <p>where \(q_{s_t}, q_{a_t}\) are positive over \(\mathcal S\) and \(\mathcal A\) respectively, \(\hat Q_{soft}^{\bar{\theta}} (s_t,a_t) =r_t+\gamma \mathbb E_{s_{t+1} \sim p_s} \left[ V_{soft}^\bar{\theta} (s_{t+1})\right]\) is a target Q-value, with \(V_{soft}^\bar{\theta} (s_{t+1})\) given by (\ref{eqn:prac-value}) and \(\theta\) being replaced by the target parameters, \(\bar{\theta}\) . While sampling distributions \(q_{s_t}\) and \(q_{a_t}\) can be arbitrary, real samples(=replay memories) are used in this paper.</p> <p><br> <br></p> <h3 id="approximate-sampling-and-stein-variational-gradient-descent-svgd">Approximate Sampling and Stein Variational Gradient Descent (SVGD)</h3> <p>To handle problem 2, this paper used a <strong>sampling network</strong> based on Stein variational gradient descent (SVGD) and amortized SVGD. This sampling network has intriguing properties: <strong>1) extremly fast sample generation, 2) accurate posterior distribution of an EBM</strong> and <strong>3)analogous to actor-critic algorithm(Details in Appendix B)</strong>. State-conditioned stochastic neural network \(a_t=f^\phi(\xi;s_t)\) , parametrized py \(\phi\) , that maps noise samples \(\xi\) is the form of network. The induced distribution of the actions are denoted as \(\pi^\phi(a_t \mid s_t)\) , and the objective is to find parameters \(\phi\) so that the <strong>induced distribution approximates the energy-based distribution in terms of the KL divergence</strong></p> \[\begin{equation} J_\pi(\phi;s_t)=\mathrm{D_KL}\left(\pi^\phi(\cdot \mid s_t)\ \Vert \ \mathrm{exp} \left(\frac{1}{\alpha}\left(Q_{soft}^\theta(s_t,\cdot)-V_{soft}^\theta \right)\right)\right). \end{equation}\] <p>SVGD provides the optimal direction in the reproducing kernel Hilbert space of \(\kappa\) as \(\delta f^\phi\) (Algorithm 1. line 21-23, averaged over M sample actions, total K action sets):</p> \[\begin{equation} \Delta f^\phi(\cdot;s_t)=\mathbb E_{a_t \sim \pi^\phi} \left[ \kappa(a_t, f^\phi(\cdot;s_t)) \nabla_{a'}\ Q_{soft}^\theta(s_t,a') \mid_{a'=a_t} +\ \alpha \nabla_{a'} \ \kappa(a',f^\phi(\cdot;s_t))\mid_{a'=a_t} \right]. \end{equation}\] <p>By the assumtion \({\partial J_\pi \over\partial a_t} \propto \Delta f^\phi\), we can use the chain rule and backpropagate SVGD into policy network according to (Algorithm 1. line 24-25, averaged over K action sets)</p> \[\begin{equation} {\partial J_\pi (\phi;s_t) \over \partial \phi} \propto \mathbb E_\xi \left[ \Delta f^\phi(\xi;s_t) {\partial f^\phi(\xi;s_t) \over \partial \phi} \right]. \end{equation}\] <p>Details of updating policy parameters are described in Appendix C.1.</p> <p><br> <br></p> <h3 id="algorithm-code--additional-info">Algorithm Code &amp; Additional Info</h3> <div class="container"> <div class="row justify-content-xl-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/SoftQLearning/SQL-algorithm.PNG-480.webp 480w, /assets/img/SoftQLearning/SQL-algorithm.PNG-800.webp 800w, /assets/img/SoftQLearning/SQL-algorithm.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/SoftQLearning/SQL-algorithm.PNG" class="img-fluid" width="100%" height="auto" title="SQL Algorithm" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from Reinforcement Learning with Deep Energy-Based Policies </div> </div> <p><em>IMO, target policy parameters \(\bar{\phi}\) are intended to sample an action in line 7.</em></p> <p><em>update_interval in line 27 freezes target parameters to stabilize training.</em></p> <p><br> <br> <br></p> <hr> <h1 id="experiments">Experiments</h1> <p><strong>What to Figure Out ?</strong></p> <ol> <li>Does their soft Q-learning method <strong>accurately capture a multi-modal policy distribution?</strong> </li> <li>Can soft Q-learning with energy-based policies <strong>aid exploration for complex tasks that require tracking multiple modes?</strong> </li> <li>Can a maximum entropy policy serve as <strong>a good initialization for fine-tuning on different tasks, when compared to pretraining with a standard deterministic objective?</strong> </li> </ol> <h3 id="didactidc-example-multi-goal-environment">Didactidc Example: Multi-Goal Environment</h3> <div class="container"> <div class="row justify-content-xl-center"> <div class="col-8"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/SoftQLearning/multi-goal-env.PNG-480.webp 480w, /assets/img/SoftQLearning/multi-goal-env.PNG-800.webp 800w, /assets/img/SoftQLearning/multi-goal-env.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/SoftQLearning/multi-goal-env.PNG" class="img-fluid" width="100%" height="auto" title="multi-goal-env" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from Reinforcement Learning with Deep Energy-Based Policies </div> </div> <p>Illustration of 2D multi-goal environment. Left: trajectories from a policy learned with soft Q-learning. Right: Q-values at three selected states and 2D velocity of action samples. The stochastic policy samples actions closely following the energy landscape, hence <strong>learning diverse trajectories that lead to all four goals</strong>. In comparison, a policy trained with DDPG randomly <strong>commits to a single goal</strong>.</p> <p><br> <br></p> <h3 id="learning-multi-modal-policies-for-exploration">Learning Multi-Modal Policies for Exploration</h3> <div class="container"> <div class="row justify-content-xl-center"> <div class="col-6"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/SoftQLearning/multi-modal-exp.PNG-480.webp 480w, /assets/img/SoftQLearning/multi-modal-exp.PNG-800.webp 800w, /assets/img/SoftQLearning/multi-modal-exp.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/SoftQLearning/multi-modal-exp.PNG" width="100%" height="auto" title="multi-modal-exp" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure from Reinforcement Learning with Deep Energy-Based Policies </div> </div> <p>During the learning process, it is often best <strong>to keep trying multiple available options until the agent is confident that one of them is the best.</strong> The results on swimmer snake task and the quadrupedal robot maze task show that all runs of Soft Q-Learning method cross the threshold line, <strong>acquiring the more optimal strategy</strong>, while some runs of DDPG do not.</p> <p><br> <br></p> <h3 id="accelerating-training-on-complex-tasks-with-pretrained-maximum-entropy-policies">Accelerating Training on Complex Tasks with Pretrained Maximum Entropy Policies</h3> <p>Aims to find out how energy based policies can be trained with <strong>fairly broad objectives to produce an initializer</strong> for more quickly learning more specific tasks. The pretraining phase involves learning to locomote in an arbitrary direction, with a reward that simply equals the speed of the center of mass. Details of the pretraining are described in Figure 7 in Appendix D.3.</p> <div class="container"> <div class="row justify-content-xl-center"> <div class="col-8"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/SoftQLearning/pretrain-SQL.PNG-480.webp 480w, /assets/img/SoftQLearning/pretrain-SQL.PNG-800.webp 800w, /assets/img/SoftQLearning/pretrain-SQL.PNG-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/SoftQLearning/pretrain-SQL.PNG" class="img-fluid" width="100%" height="auto" title="pretrain-SQL" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> figure from Reinforcement Learning with Deep Energy-Based Policies </div> </div> <p>As the plots show, the pretrained policy gives a good initialization to learn the behaviors in the test environments more quickly than training a policy with DDPG from a random initialization.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Wooyeol Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script async src="https://rum.cronitor.io/script.js"></script> <script>window.cronitor=window.cronitor||function(){(window.cronitor.q=window.cronitor.q||[]).push(arguments)},cronitor("config",{clientKey:""});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>