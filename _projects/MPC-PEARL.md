---
layout: page
title: MPC-PEARL
description: Review on "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments"
img: assets/img/MPC-PEARL/MPC-PEARL-thumbnail.PNG
importance: 5
category: papers review
---

TL;DR:
- MPC-PEARL: a novel combination of meta-RL and MPC with **event-triggered probabilistic switching between the two modules**.
- **Event-triggered switching** makes up for ineffective behaviors of PEARL with MPC, **encouraging exploration of potentially high-reward regions**.
- **Randomization layer** induces MPC-PEARL to learn frequently from actions generated by MPC, thereby **complementing suboptimal MPC actions caused by the limited prediction horizon**.
- **GPR** and **CVaR constraint** are used in MPC **to build safe region constraint with unknown motion of dynamic obstacles**.
- An online adaptdation scheme enables the robot **to infer and adapt to a new task within a single trajectory**.

<br/>
<br/>
<br/>

--------

# Introduction
<br/>

### Limitations of meta-RL & MPC

As it is crucial for mobile robots to adapt quickly to environmental changes, meta-RL algorithms (e.g. PEARL) are suitable for robot navigation problems. However, the **meta-learned policy may be conservative** in some situations. Another sequential decision-making tool, which is MPC, can be used to infer unknown parts of the system or environmental model with various ML techniques. But, learning-based MPC techniques are **computationally demanding** and MPC optimizes an action sequence within a **fixed short horizon, causing suboptimal, myopic behaviors in general**.

<br/>
<br/>

### Motivation of MPC-PEARL

There have been a few attemps to use MPC in model-based meta-RL or meta-learning methods, such as using learned MPC in real-time decision making. However, none of them utilizes the actions generated by MPC in the **training stage**. Motivated by this observation, the authors proposed a systematic approach to **infusing MPC into the meta-RL training process** for mobile robots in environments cluttered with moving obstacles. 

<br/>
<br/>
<br/>

-------

# Preliminaries
<br/>

### Motion Control in Dynamic Environments

<div class="row justify-content-center">
    <div class="col-6">
        {% include figure.html path="assets/img/MPC-PEARL/MPC-PEARL-dynamic-system.PNG" title="configuration of the motion control problem" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments"
</div>

**The motions of the robot and obstacles**

- The motion of the robot is modeled by the following discrete-time nonlinear system:

  $$
  \begin{equation}
  x_{t+1} = f(x_t, u_t)
  \end{equation}
  $$
  , where $$ x_t \in \mathcal X \subseteq \mathbb R^{n_x} $$ and $$ u_t \in \mathcal U \subseteq \mathbb R^{n_u} $$ are the robot's state and control input at stage t.
- state of dynamic obstacles: $$ x_{t,i}^d \in \mathbb R^{n_d}, i=1, ..., N_d $$
- state of static obstacles: $$ x_i^s \in \mathbb R^{n_s}, i=1,...,N_s $$
- As a common practice, the obstacles' motion can be tracked with high accuracy (fully observability assumption).
<br/>

**Definition of the motion control problem**

- The motion control problem: MDP tuple $$ \mathcal T := \left( \mathcal{S,U} , P, c, \gamma \right) $$ , where $$ \mathcal S \subseteq \mathbb R^{n_x+n_dN_d+n_sN_s} $$ and $$ \mathcal U $$ are the state and action spaces, respectively, $$ c$$ iis a stage-wise cost function of interest. 
- The MDP state: $$ s_t := \left( x_t,x_{t,1}^d, ..., x_{t,N_d}^d, x_1^s,..., x_{N_s}^s \right) $$ , concatenating the robot state and the states of the static and dynamic obstacles. 
- The stage-cost function $$ c $$ is chosen as follows:

  $$
  \begin{equation}
  c(s_t,u_t) := l(x_t,u_t) + w_d \mathbf 1_{ \lbrace x_t \notin \mathcal X_t^d \rbrace } + w_s \mathbf 1_{ \lbrace x_t \notin \mathcal X^s \rbrace } - w_g \mathbf 1_{ \lbrace x_t \in \mathcal X^g \rbrace } , 
  \end{equation}
  $$

  where the loss function $$ l $$ measures the control performance, $$ \mathcal X_t^d $$ and $$ \mathcal X^s $$ denote the safe regions with respect to dynamic and static obstacles to be defined.
- $$ \mathcal X_t^d := \lbrace x \in \mathbb R^{n_x} \mid h_d(x,x_{t,i}^d) \geq \epsilon_i^d, i=1,...N_d \rbrace $$ : The safety function is $$ h_d(x,x_i^d) := \lVert p^r -p_i^d \rVert_2 $$ , where $$ p^r $$ and $$ p_i^d $$ represent the center of gravities of the robot and dynamic obstacle $$ i $$ , respectively. The threshold is $$ \epsilon_i^d = r^r+r_i^d+r^\text{safe} $$ , where $$ r^r $$ and $$ r_i^d $$ are the radii of balls covering the robot and the obstacle respectively. $$ \mathcal X^s $$ can be designed in a similar manner.
- $$ \mathcal X^g := \lbrace x \in \mathbb R^{n_x} \mid \lVert p^r -p_\text{goal} \rVert_2 \leq \epsilon^g \rbrace $$ : incentivizes the robot to reach a neighborhood of the goal point.

<br/>
<br/>

### Off-Policy Meta-Reinforcement Learning

As the motion pattern of dynamic obstacles and the configuration of static obstacles (transition function $$ P $$ and cost function $$ c $$ ) vary, it is desired to train the robot via meta-RL. To enhance sample efficiency in meta-RL, adopt [`PEARL`](https://thisiswooyeol.github.io/projects/PEARL/), which is a state-of-the-art off-policy meta-RL algorithm. (You can see detailed explanations on `PEARL` algorithm at the link above.)

<div class="row justify-content-center">
    <div class="col-6">
        {% include figure.html path="assets/img/MPC-PEARL/PEARL-on-navigation-problem.PNG" title="PEARL on robot navigation task" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments"
</div>

Fig. 2 shows how the PEARL policy operates in the robot navigation task. This example indicates that **PEARL policies may induce overly conservative behaviors** to bypass regions cluttered with moving obstacles.

<br/>
<br/>
<br/>

-------

# Infusing MPC into Meta-RL
<br/>

<div class="row justify-content-center">
    <div class="col-6">
        {% include figure.html path="assets/img/MPC-PEARL/Overview-of-MPC-PEARL.PNG" title="Overview of MPC-PEARL" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments"
</div>

To resolve the limited nagivation performance of PEARL, learning-based MPC is combined to **provide it with transition samples in case a predefined event occurs**. (The authors considered the following two events that are ineffectively handled by PEARL: (Event 1) collision with a dynamic obstacle, and (Event 2) reaching the neighboring region of the goal point.) If the MPC module is activated, a carefully designed MPC problem is solved using **GPR result of inferring the motion of obstacles**. Otherwise, the original PEARL algorithm chooses an action. Note that **the randomization layer induces MPC-PEARL to learn frequently from actions generated by MPC**.

<br/>
<br/>

### MPC-PEARL Algorithm
<br/>

<div class="row justify-content-center">
    <div class="col-6">
        {% include figure.html path="assets/img/MPC-PEARL/MPC-PEARL-meta-training.PNG" title="MPC-PEARL meta-training" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments"
</div>

- (line 4-17) Transition samples are collected for each task $$ \mathcal T^i $$ to build a task-specific replay-buffer $$ \mathcal H^i $$ .
- (line 9-11) A carefully designed MPC predicts the motion of dynamic obstacles via GPR and to avoid collisions via CVaR constraints.
- (line 20) $$ \mathcal S_c $$ is chosen to generate transition samples uniformly from the most recent data collection stage.
- (line 22-23) The policy is trained using [`soft actor-critic (SAC)`](https://thisiswooyeol.github.io/projects/SoftActorCritic/) and the context encoder network is trained as in [`PEARL`](https://thisiswooyeol.github.io/projects/PEARL/).

<br/>

<div class="row justify-content-center">
    <div class="col-6">
        {% include figure.html path="assets/img/MPC-PEARL/MPC-PEARL-meta-testing.PNG" title="MPC-PEARL meta-testing" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Figure from "Infusing Model Predictive Control into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments"
</div>

- By **omitting the MPC module during meta-testing**, a significant amount of **computation time can be saved**.
- (line 5-6) The online adaptation scheme, performed **within a single trajectory**, is crucial for some practical applications where **each episode is less likely to be repeated** due to its consistently evolving environments.

<br/>
<br/>

### Learning-Based MPC with CVaR Constraints

As PEARL does not use the known system dynamcis that is often available for various practical robots, authors adpoted a learning-based MPC technique that solves the given task and uses model information in a receding horizon fashion:

$$
\begin{subequations}
\begin{align}
\mathrm{min}_ {\mathbf u} & J(x_t, \mathbf u) = l_f(x_{t+K \mid t} ) + \sum_{k=0}^{K-1} l(x_{t+k \mid t} , u_{t+k \mid t} ) \\
\text{s.t.} & x_{t+k+1 \mid t} = f(x_{t+K \mid t} , u_{t+k \mid t}) \label{eqn:motion-model} \\
& x_{t \mid t} = x_t \label{eqn:3c} \\
& x_{t+k \mid t} \in \mathcal X_{t+k}^d \cap \mathcal X^s \label{eqn:3d} \\
& u_{u+k \mid t} \in \mathcal U, \label{eqn:3e}
\end{align}
\end{subequations}
$$

where $$ \mathbf u = (u_{t \mid t} , ... , u_{t+K-1 \mid t} ) $$ is a control sequence over the $$ K $$ prediction horizon, \eqref{eqn:motion-model} and \eqref{eqn:3e} must be satisfied for $$ k=0,..., K-1} and \eqref{eqn:3d} (safe region constraint) must hold for $$ k=0,...K $$ . Unfortunately, the MPC problem in the above form is impossible to solve because in general **the safe region $$ \mathcal X_{t+k}^d $$ is unknown**; it includes information about the future motion of dynamic obstacles. Here are how the authors solved this problem.

<br/>

**Learning the motion of obstacles via GPR**

